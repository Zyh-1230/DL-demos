{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "989a13f3-2f47-4d67-bd5d-e727cd8bf110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "from tensorflow.keras import layers, datasets, Sequential, Model, optimizers\n",
    "from tensorflow.keras.layers import LeakyReLU, UpSampling2D, Conv2D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys, os, pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501d18b8-78f7-4800-a1c4-43e0c0d0b2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (28, 28, 1) # MNIST图像尺寸\n",
    "latent_dim = 200 # 噪声向量的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "984b2c7f-10a6-4fc6-9656-5b96cf259453",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('images', exist_ok=True) # 创建保存图像的文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f2923ba-4346-45e8-97e4-27abcb3de935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    # ======================================= #\n",
    "    #     生成器，输入一串随机噪声向量生成图片\n",
    "    # ======================================= #\n",
    "    model = Sequential([\n",
    "        layers.Dense(256, input_dim=latent_dim),\n",
    "        layers.LeakyReLU(alpha=0.2), #相比于传统的ReLU:f(x) = max(0, x)--->LeakyReLU:f(x)=max(alpha*x, x) alpha是一个很小的正数 解决了神经元死亡问题 可帮助梯度更好地流动\n",
    "        layers.BatchNormalization(momentum=0.8),\n",
    "\n",
    "        layers.Dense(512),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.BatchNormalization(momentum=0.8),\n",
    "\n",
    "        layers.Dense(1024),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.BatchNormalization(momentum=0.8),\n",
    "\n",
    "        layers.Dense(np.prod(img_shape), activation='tanh'),\n",
    "        layers.Reshape(img_shape)\n",
    "    ])\n",
    "\n",
    "    noise = layers.Input(shape=(latent_dim,))\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7645c497-885a-4f0d-aa69-f5306a9a8b81",
   "metadata": {},
   "source": [
    "def build_generator():\n",
    "    model = Sequential([\n",
    "        layers.Dense(7*7*256, use_bias=False, input_shape=(latent_dim,)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "\n",
    "        layers.Reshape((7, 7, 256)),\n",
    "\n",
    "        layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False), # strides?\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "\n",
    "        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "\n",
    "        layers.Conv2DTranspose(1, (5, 5), strides=(1, 1), padding='same', activation='tanh') # strides?\n",
    "        \n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e16c7ed-7660-4680-b8f8-97cf1296008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    # ===================================== #\n",
    "    #   鉴别器，对输入的图片进行判别真假\n",
    "    # ===================================== #\n",
    "    model = Sequential([\n",
    "        layers.Flatten(input_shape=img_shape),\n",
    "        layers.Dense(512),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dense(256),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dense(1, activation='sigmoid') # 输出真图概率\n",
    "    ])\n",
    "\n",
    "    img = layers.Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ff5a38d-0fae-4733-9082-081bfbbe5608",
   "metadata": {},
   "source": [
    "def build_discriminator():\n",
    "    model = Sequential([\n",
    "        layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=img_shape),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        layers.Conv2D(256, (5, 5), strides=(1, 1), padding='same'),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "784222f3-bfef-4a87-bc0f-2edc29a8a481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "discriminator = build_discriminator()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "generator = build_generator()\n",
    "\n",
    "discriminator.trainable = False # 停止鉴别器学习\n",
    "# 生成器\n",
    "gan_input = layers.Input(shape=(latent_dim,))\n",
    "img = generator(gan_input)\n",
    "validity = discriminator(img)\n",
    "combined = Model(gan_input, validity) # 输入噪声--造假--鉴别\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer) # target: 让discriminator将fake鉴别为true"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b660dbe-090a-4925-9b47-0c76c7e2ff64",
   "metadata": {},
   "source": [
    "discriminator = build_discriminator()\n",
    "\n",
    "g_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.5, clipvalue=1.0)\n",
    "d_optimizer = tf.keras.optimizers.Adam(0.0001, beta_1=0.5, clipvalue=1.0)\n",
    "\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "                      optimizer=d_optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "generator = build_generator()\n",
    "\n",
    "discriminator.trainable = False # 停止鉴别器学习\n",
    "# 生成器\n",
    "gan_input = layers.Input(shape=(latent_dim,))\n",
    "img = generator(gan_input)\n",
    "validity = discriminator(img)\n",
    "combined = Model(gan_input, validity) # 输入噪声--造假--鉴别\n",
    "combined.compile(loss='binary_crossentropy', optimizer=g_optimizer) # target: 让discriminator将fake鉴别为true"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77a1ceb9-399b-4c0e-9823-f4c05ae713b1",
   "metadata": {},
   "source": [
    "# 在模型构建后添加权重初始化\n",
    "def initialize_weights(model):\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            layer.kernel.assign(layer.kernel_initializer(tf.shape(layer.kernel)))\n",
    "        if hasattr(layer, 'bias_initializer') and hasattr(layer, 'bias'):\n",
    "            layer.bias.assign(layer.bias_initializer(tf.shape(layer.bias)))\n",
    "\n",
    "# 初始化权重\n",
    "initialize_weights(generator)\n",
    "initialize_weights(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae77d6d7-492f-4b23-8994-f5dcbdb0d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch):\n",
    "    '''\n",
    "    saving images\n",
    "    '''\n",
    "    row, col = 4, 4\n",
    "    noise = np.random.normal(0, 1, (row*col, latent_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    fig, axs = plt.subplots(row, col)\n",
    "    cnt = 0\n",
    "    for i in range(row):\n",
    "        for j in range(col):\n",
    "            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig('images/%05d.png' % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f96b71a0-2eeb-4995-9732-7802d3f559a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, batch_size=128, sample_interval=50):\n",
    "    # 加载并预处理MNIST数据\n",
    "    (train_images,_), (_,_) = tf.keras.datasets.mnist.load_data() \n",
    "    train_images = (train_images - 127.5) / 127.5 # 归一化到[-1, 1] 适用于tanh激活函数的生成器输出层\n",
    "    train_images = np.expand_dims(train_images, axis=3)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # discriminator.trainable = True\n",
    "        \n",
    "        # 训练判别器\n",
    "        idx = np.random.randint(0, train_images.shape[0], batch_size)\n",
    "        imgs = train_images[idx] # 真实图像\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        gen_imgs = generator.predict(noise, verbose=0) # 生成图像 verbose=0 不显示任何进度信息 =1显示进度条 =2只显示每个epoch信息\n",
    "\n",
    "        true = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "        d_loss_true = discriminator.train_on_batch(imgs, true) # 真图标签为1\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake) # 假图标签为0\n",
    "        d_loss = 0.5 * np.add(d_loss_true, d_loss_fake)\n",
    "        \n",
    "        # 训练生成器\n",
    "        # discriminator.trainable = False\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "        g_loss = combined.train_on_batch(noise, true) # 期望输出1 实际输出是判别器的真实判断概率值 计算交叉熵损失\n",
    "        \n",
    "        print('%d [D loss: %f, acc.: %.2f%%] [G loss: %f]' % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "        # if epoch % 10 == 0:\n",
    "        #     print(f\"{epoch} [D loss: {d_loss[0]:.6f}, acc.: {100*d_loss[1]:.2f}%] [G loss: {g_loss:.6f}]\")\n",
    "            \n",
    "        # 打印损失并保存样本图像\n",
    "        if epoch % sample_interval == 0:\n",
    "            sample_images(epoch)\n",
    "            #if epoch % 100 == 0:\n",
    "            #    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57209f6a-a6e0-493a-998f-f086b7805565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.947463, acc.: 29.10%] [G loss: 0.867090]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:83: UserWarning: The model does not have any trainable weights.\n",
      "  warnings.warn(\"The model does not have any trainable weights.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "1 [D loss: 0.896657, acc.: 42.12%] [G loss: 0.846413]\n",
      "2 [D loss: 0.893049, acc.: 42.14%] [G loss: 0.833053]\n",
      "3 [D loss: 0.891344, acc.: 40.05%] [G loss: 0.816624]\n",
      "4 [D loss: 0.891672, acc.: 37.64%] [G loss: 0.804610]\n",
      "5 [D loss: 0.896650, acc.: 35.38%] [G loss: 0.791498]\n",
      "6 [D loss: 0.899874, acc.: 33.95%] [G loss: 0.775906]\n",
      "7 [D loss: 0.902210, acc.: 32.62%] [G loss: 0.764127]\n",
      "8 [D loss: 0.906665, acc.: 31.11%] [G loss: 0.750302]\n",
      "9 [D loss: 0.911955, acc.: 29.63%] [G loss: 0.737507]\n",
      "10 [D loss: 0.917282, acc.: 28.23%] [G loss: 0.725954]\n",
      "11 [D loss: 0.922218, acc.: 27.04%] [G loss: 0.713138]\n",
      "12 [D loss: 0.926288, acc.: 25.98%] [G loss: 0.701705]\n",
      "13 [D loss: 0.931378, acc.: 24.97%] [G loss: 0.689989]\n",
      "14 [D loss: 0.937610, acc.: 23.91%] [G loss: 0.678642]\n",
      "15 [D loss: 0.943731, acc.: 22.96%] [G loss: 0.667809]\n",
      "16 [D loss: 0.949052, acc.: 22.09%] [G loss: 0.657225]\n",
      "17 [D loss: 0.956511, acc.: 21.16%] [G loss: 0.647309]\n",
      "18 [D loss: 0.964223, acc.: 20.34%] [G loss: 0.636938]\n",
      "19 [D loss: 0.970223, acc.: 19.62%] [G loss: 0.626313]\n",
      "20 [D loss: 0.976065, acc.: 18.97%] [G loss: 0.617118]\n",
      "21 [D loss: 0.982728, acc.: 18.30%] [G loss: 0.607370]\n",
      "22 [D loss: 0.990110, acc.: 17.67%] [G loss: 0.598086]\n",
      "23 [D loss: 0.996263, acc.: 17.12%] [G loss: 0.589137]\n",
      "24 [D loss: 1.002648, acc.: 16.54%] [G loss: 0.580566]\n",
      "25 [D loss: 1.008696, acc.: 16.05%] [G loss: 0.572032]\n",
      "26 [D loss: 1.016029, acc.: 15.54%] [G loss: 0.563161]\n",
      "27 [D loss: 1.022842, acc.: 15.06%] [G loss: 0.554881]\n",
      "28 [D loss: 1.029202, acc.: 14.65%] [G loss: 0.546911]\n",
      "29 [D loss: 1.035425, acc.: 14.33%] [G loss: 0.538726]\n",
      "30 [D loss: 1.042210, acc.: 13.99%] [G loss: 0.531365]\n",
      "31 [D loss: 1.049302, acc.: 13.66%] [G loss: 0.523877]\n",
      "32 [D loss: 1.056809, acc.: 13.32%] [G loss: 0.516548]\n",
      "33 [D loss: 1.063933, acc.: 12.97%] [G loss: 0.509396]\n",
      "34 [D loss: 1.071024, acc.: 12.71%] [G loss: 0.502206]\n",
      "35 [D loss: 1.077996, acc.: 12.43%] [G loss: 0.495294]\n",
      "36 [D loss: 1.084874, acc.: 12.18%] [G loss: 0.488542]\n",
      "37 [D loss: 1.092311, acc.: 11.93%] [G loss: 0.482074]\n",
      "38 [D loss: 1.099544, acc.: 11.69%] [G loss: 0.475536]\n",
      "39 [D loss: 1.106534, acc.: 11.49%] [G loss: 0.469187]\n",
      "40 [D loss: 1.113854, acc.: 11.28%] [G loss: 0.463055]\n",
      "41 [D loss: 1.121289, acc.: 11.07%] [G loss: 0.457068]\n",
      "42 [D loss: 1.128851, acc.: 10.89%] [G loss: 0.451251]\n",
      "43 [D loss: 1.135838, acc.: 10.70%] [G loss: 0.445596]\n",
      "44 [D loss: 1.142475, acc.: 10.50%] [G loss: 0.439909]\n",
      "45 [D loss: 1.149165, acc.: 10.34%] [G loss: 0.434594]\n",
      "46 [D loss: 1.156120, acc.: 10.18%] [G loss: 0.429183]\n",
      "47 [D loss: 1.162857, acc.: 10.03%] [G loss: 0.424082]\n",
      "48 [D loss: 1.169859, acc.: 9.88%] [G loss: 0.419059]\n",
      "49 [D loss: 1.176900, acc.: 9.76%] [G loss: 0.414094]\n",
      "50 [D loss: 1.184117, acc.: 9.63%] [G loss: 0.409216]\n",
      "51 [D loss: 1.191276, acc.: 9.47%] [G loss: 0.404403]\n",
      "52 [D loss: 1.198279, acc.: 9.34%] [G loss: 0.399925]\n",
      "53 [D loss: 1.205514, acc.: 9.23%] [G loss: 0.395339]\n",
      "54 [D loss: 1.212709, acc.: 9.11%] [G loss: 0.390768]\n",
      "55 [D loss: 1.219929, acc.: 8.99%] [G loss: 0.386259]\n",
      "56 [D loss: 1.226427, acc.: 8.91%] [G loss: 0.381946]\n",
      "57 [D loss: 1.233443, acc.: 8.79%] [G loss: 0.377703]\n",
      "58 [D loss: 1.240309, acc.: 8.68%] [G loss: 0.373479]\n",
      "59 [D loss: 1.246912, acc.: 8.57%] [G loss: 0.369502]\n",
      "60 [D loss: 1.253977, acc.: 8.48%] [G loss: 0.365501]\n",
      "61 [D loss: 1.260976, acc.: 8.39%] [G loss: 0.361573]\n",
      "62 [D loss: 1.267510, acc.: 8.29%] [G loss: 0.357747]\n",
      "63 [D loss: 1.274151, acc.: 8.20%] [G loss: 0.353943]\n",
      "64 [D loss: 1.280783, acc.: 8.11%] [G loss: 0.350289]\n",
      "65 [D loss: 1.287564, acc.: 8.02%] [G loss: 0.346724]\n",
      "66 [D loss: 1.293811, acc.: 7.96%] [G loss: 0.343198]\n",
      "67 [D loss: 1.300227, acc.: 7.89%] [G loss: 0.339736]\n",
      "68 [D loss: 1.306993, acc.: 7.82%] [G loss: 0.336325]\n",
      "69 [D loss: 1.313478, acc.: 7.75%] [G loss: 0.332979]\n",
      "70 [D loss: 1.319858, acc.: 7.68%] [G loss: 0.329681]\n",
      "71 [D loss: 1.326326, acc.: 7.61%] [G loss: 0.326419]\n",
      "72 [D loss: 1.332596, acc.: 7.55%] [G loss: 0.323324]\n",
      "73 [D loss: 1.338785, acc.: 7.50%] [G loss: 0.320230]\n",
      "74 [D loss: 1.345002, acc.: 7.43%] [G loss: 0.317256]\n",
      "75 [D loss: 1.351389, acc.: 7.38%] [G loss: 0.314225]\n",
      "76 [D loss: 1.357535, acc.: 7.32%] [G loss: 0.311268]\n",
      "77 [D loss: 1.363865, acc.: 7.25%] [G loss: 0.308401]\n",
      "78 [D loss: 1.370211, acc.: 7.20%] [G loss: 0.305599]\n",
      "79 [D loss: 1.376273, acc.: 7.15%] [G loss: 0.302889]\n",
      "80 [D loss: 1.382243, acc.: 7.10%] [G loss: 0.300162]\n",
      "81 [D loss: 1.388406, acc.: 7.05%] [G loss: 0.297450]\n",
      "82 [D loss: 1.394319, acc.: 7.00%] [G loss: 0.294857]\n",
      "83 [D loss: 1.400239, acc.: 6.94%] [G loss: 0.292288]\n",
      "84 [D loss: 1.406322, acc.: 6.89%] [G loss: 0.289723]\n",
      "85 [D loss: 1.412106, acc.: 6.83%] [G loss: 0.287221]\n",
      "86 [D loss: 1.418083, acc.: 6.79%] [G loss: 0.284757]\n",
      "87 [D loss: 1.424002, acc.: 6.77%] [G loss: 0.282357]\n",
      "88 [D loss: 1.429871, acc.: 6.72%] [G loss: 0.280000]\n",
      "89 [D loss: 1.435559, acc.: 6.68%] [G loss: 0.277639]\n",
      "90 [D loss: 1.441127, acc.: 6.63%] [G loss: 0.275361]\n",
      "91 [D loss: 1.446791, acc.: 6.59%] [G loss: 0.273107]\n",
      "92 [D loss: 1.452563, acc.: 6.56%] [G loss: 0.270885]\n",
      "93 [D loss: 1.458209, acc.: 6.54%] [G loss: 0.268682]\n",
      "94 [D loss: 1.463632, acc.: 6.49%] [G loss: 0.266511]\n",
      "95 [D loss: 1.468857, acc.: 6.46%] [G loss: 0.264444]\n",
      "96 [D loss: 1.474217, acc.: 6.42%] [G loss: 0.262401]\n",
      "97 [D loss: 1.479758, acc.: 6.39%] [G loss: 0.260331]\n",
      "98 [D loss: 1.485309, acc.: 6.35%] [G loss: 0.258330]\n",
      "99 [D loss: 1.491043, acc.: 6.32%] [G loss: 0.256331]\n",
      "100 [D loss: 1.496722, acc.: 6.28%] [G loss: 0.254377]\n",
      "101 [D loss: 1.501931, acc.: 6.25%] [G loss: 0.252444]\n",
      "102 [D loss: 1.507189, acc.: 6.22%] [G loss: 0.250520]\n",
      "103 [D loss: 1.512480, acc.: 6.19%] [G loss: 0.248651]\n",
      "104 [D loss: 1.517651, acc.: 6.17%] [G loss: 0.246807]\n",
      "105 [D loss: 1.522671, acc.: 6.14%] [G loss: 0.245001]\n",
      "106 [D loss: 1.527739, acc.: 6.11%] [G loss: 0.243223]\n",
      "107 [D loss: 1.533349, acc.: 6.07%] [G loss: 0.241458]\n",
      "108 [D loss: 1.538591, acc.: 6.04%] [G loss: 0.239731]\n",
      "109 [D loss: 1.543619, acc.: 6.02%] [G loss: 0.238016]\n",
      "110 [D loss: 1.548812, acc.: 5.98%] [G loss: 0.236321]\n",
      "111 [D loss: 1.554000, acc.: 5.96%] [G loss: 0.234661]\n",
      "112 [D loss: 1.559081, acc.: 5.92%] [G loss: 0.233017]\n",
      "113 [D loss: 1.564047, acc.: 5.90%] [G loss: 0.231391]\n",
      "114 [D loss: 1.569116, acc.: 5.87%] [G loss: 0.229785]\n",
      "115 [D loss: 1.574299, acc.: 5.85%] [G loss: 0.228238]\n",
      "116 [D loss: 1.579377, acc.: 5.82%] [G loss: 0.226654]\n",
      "117 [D loss: 1.584212, acc.: 5.80%] [G loss: 0.225117]\n",
      "118 [D loss: 1.589167, acc.: 5.78%] [G loss: 0.223602]\n",
      "119 [D loss: 1.594125, acc.: 5.75%] [G loss: 0.222105]\n",
      "120 [D loss: 1.599124, acc.: 5.72%] [G loss: 0.220640]\n",
      "121 [D loss: 1.603806, acc.: 5.70%] [G loss: 0.219176]\n",
      "122 [D loss: 1.608539, acc.: 5.68%] [G loss: 0.217755]\n",
      "123 [D loss: 1.613139, acc.: 5.66%] [G loss: 0.216357]\n",
      "124 [D loss: 1.617683, acc.: 5.63%] [G loss: 0.214966]\n",
      "125 [D loss: 1.622123, acc.: 5.61%] [G loss: 0.213585]\n",
      "126 [D loss: 1.626617, acc.: 5.59%] [G loss: 0.212236]\n",
      "127 [D loss: 1.631328, acc.: 5.56%] [G loss: 0.210908]\n",
      "128 [D loss: 1.635989, acc.: 5.53%] [G loss: 0.209581]\n",
      "129 [D loss: 1.640809, acc.: 5.51%] [G loss: 0.208261]\n",
      "130 [D loss: 1.645522, acc.: 5.48%] [G loss: 0.206959]\n",
      "131 [D loss: 1.650286, acc.: 5.46%] [G loss: 0.205701]\n",
      "132 [D loss: 1.654842, acc.: 5.44%] [G loss: 0.204433]\n",
      "133 [D loss: 1.659338, acc.: 5.42%] [G loss: 0.203185]\n",
      "134 [D loss: 1.663776, acc.: 5.40%] [G loss: 0.201947]\n",
      "135 [D loss: 1.668155, acc.: 5.38%] [G loss: 0.200727]\n",
      "136 [D loss: 1.672523, acc.: 5.37%] [G loss: 0.199518]\n",
      "137 [D loss: 1.676947, acc.: 5.34%] [G loss: 0.198328]\n",
      "138 [D loss: 1.681157, acc.: 5.33%] [G loss: 0.197154]\n",
      "139 [D loss: 1.685495, acc.: 5.32%] [G loss: 0.195999]\n",
      "140 [D loss: 1.689857, acc.: 5.30%] [G loss: 0.194847]\n",
      "141 [D loss: 1.694191, acc.: 5.28%] [G loss: 0.193729]\n",
      "142 [D loss: 1.698643, acc.: 5.26%] [G loss: 0.192618]\n",
      "143 [D loss: 1.703084, acc.: 5.24%] [G loss: 0.191519]\n",
      "144 [D loss: 1.707424, acc.: 5.22%] [G loss: 0.190439]\n",
      "145 [D loss: 1.711748, acc.: 5.20%] [G loss: 0.189368]\n",
      "146 [D loss: 1.716124, acc.: 5.18%] [G loss: 0.188304]\n",
      "147 [D loss: 1.720424, acc.: 5.16%] [G loss: 0.187253]\n",
      "148 [D loss: 1.724664, acc.: 5.15%] [G loss: 0.186223]\n",
      "149 [D loss: 1.728906, acc.: 5.12%] [G loss: 0.185190]\n",
      "150 [D loss: 1.733114, acc.: 5.10%] [G loss: 0.184161]\n",
      "151 [D loss: 1.737197, acc.: 5.10%] [G loss: 0.183150]\n",
      "152 [D loss: 1.741296, acc.: 5.08%] [G loss: 0.182159]\n",
      "153 [D loss: 1.745229, acc.: 5.07%] [G loss: 0.181173]\n",
      "154 [D loss: 1.749348, acc.: 5.05%] [G loss: 0.180210]\n",
      "155 [D loss: 1.753416, acc.: 5.04%] [G loss: 0.179241]\n",
      "156 [D loss: 1.757174, acc.: 5.03%] [G loss: 0.178295]\n",
      "157 [D loss: 1.761123, acc.: 5.02%] [G loss: 0.177359]\n",
      "158 [D loss: 1.765385, acc.: 5.01%] [G loss: 0.176425]\n",
      "159 [D loss: 1.769482, acc.: 5.00%] [G loss: 0.175501]\n",
      "160 [D loss: 1.773624, acc.: 4.98%] [G loss: 0.174585]\n",
      "161 [D loss: 1.777487, acc.: 4.97%] [G loss: 0.173678]\n",
      "162 [D loss: 1.781543, acc.: 4.95%] [G loss: 0.172784]\n",
      "163 [D loss: 1.785519, acc.: 4.94%] [G loss: 0.171892]\n",
      "164 [D loss: 1.789362, acc.: 4.92%] [G loss: 0.171021]\n",
      "165 [D loss: 1.793298, acc.: 4.92%] [G loss: 0.170152]\n",
      "166 [D loss: 1.797209, acc.: 4.91%] [G loss: 0.169299]\n",
      "167 [D loss: 1.801056, acc.: 4.90%] [G loss: 0.168451]\n",
      "168 [D loss: 1.805001, acc.: 4.88%] [G loss: 0.167619]\n",
      "169 [D loss: 1.808938, acc.: 4.87%] [G loss: 0.166796]\n",
      "170 [D loss: 1.812816, acc.: 4.86%] [G loss: 0.165982]\n",
      "171 [D loss: 1.816518, acc.: 4.85%] [G loss: 0.165167]\n",
      "172 [D loss: 1.820362, acc.: 4.84%] [G loss: 0.164356]\n",
      "173 [D loss: 1.824215, acc.: 4.82%] [G loss: 0.163556]\n",
      "174 [D loss: 1.827951, acc.: 4.81%] [G loss: 0.162763]\n",
      "175 [D loss: 1.831778, acc.: 4.80%] [G loss: 0.161981]\n",
      "176 [D loss: 1.835407, acc.: 4.79%] [G loss: 0.161213]\n",
      "177 [D loss: 1.838932, acc.: 4.79%] [G loss: 0.160441]\n",
      "178 [D loss: 1.842509, acc.: 4.78%] [G loss: 0.159681]\n",
      "179 [D loss: 1.846384, acc.: 4.76%] [G loss: 0.158932]\n",
      "180 [D loss: 1.850155, acc.: 4.75%] [G loss: 0.158189]\n",
      "181 [D loss: 1.853713, acc.: 4.74%] [G loss: 0.157456]\n",
      "182 [D loss: 1.857332, acc.: 4.73%] [G loss: 0.156723]\n",
      "183 [D loss: 1.860982, acc.: 4.72%] [G loss: 0.155992]\n",
      "184 [D loss: 1.864535, acc.: 4.71%] [G loss: 0.155280]\n",
      "185 [D loss: 1.867991, acc.: 4.70%] [G loss: 0.154565]\n",
      "186 [D loss: 1.871530, acc.: 4.68%] [G loss: 0.153857]\n",
      "187 [D loss: 1.875120, acc.: 4.68%] [G loss: 0.153162]\n",
      "188 [D loss: 1.878470, acc.: 4.67%] [G loss: 0.152470]\n",
      "189 [D loss: 1.881813, acc.: 4.66%] [G loss: 0.151798]\n",
      "190 [D loss: 1.885301, acc.: 4.65%] [G loss: 0.151122]\n",
      "191 [D loss: 1.888812, acc.: 4.64%] [G loss: 0.150446]\n",
      "192 [D loss: 1.892352, acc.: 4.63%] [G loss: 0.149776]\n",
      "193 [D loss: 1.895783, acc.: 4.62%] [G loss: 0.149119]\n",
      "194 [D loss: 1.899299, acc.: 4.61%] [G loss: 0.148464]\n",
      "195 [D loss: 1.902894, acc.: 4.60%] [G loss: 0.147819]\n",
      "196 [D loss: 1.906381, acc.: 4.59%] [G loss: 0.147180]\n",
      "197 [D loss: 1.909702, acc.: 4.58%] [G loss: 0.146537]\n",
      "198 [D loss: 1.913221, acc.: 4.57%] [G loss: 0.145907]\n",
      "199 [D loss: 1.916514, acc.: 4.56%] [G loss: 0.145282]\n",
      "200 [D loss: 1.919787, acc.: 4.55%] [G loss: 0.144663]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "201 [D loss: 1.923137, acc.: 4.55%] [G loss: 0.144045]\n",
      "202 [D loss: 1.926493, acc.: 4.53%] [G loss: 0.143434]\n",
      "203 [D loss: 1.929717, acc.: 4.53%] [G loss: 0.142820]\n",
      "204 [D loss: 1.933020, acc.: 4.52%] [G loss: 0.142220]\n",
      "205 [D loss: 1.936204, acc.: 4.51%] [G loss: 0.141631]\n",
      "206 [D loss: 1.939489, acc.: 4.51%] [G loss: 0.141043]\n",
      "207 [D loss: 1.942556, acc.: 4.50%] [G loss: 0.140457]\n",
      "208 [D loss: 1.945607, acc.: 4.50%] [G loss: 0.139878]\n",
      "209 [D loss: 1.948825, acc.: 4.49%] [G loss: 0.139306]\n",
      "210 [D loss: 1.952099, acc.: 4.48%] [G loss: 0.138734]\n",
      "211 [D loss: 1.955309, acc.: 4.48%] [G loss: 0.138163]\n",
      "212 [D loss: 1.958513, acc.: 4.47%] [G loss: 0.137598]\n",
      "213 [D loss: 1.961709, acc.: 4.46%] [G loss: 0.137042]\n",
      "214 [D loss: 1.964863, acc.: 4.45%] [G loss: 0.136484]\n",
      "215 [D loss: 1.967947, acc.: 4.45%] [G loss: 0.135939]\n",
      "216 [D loss: 1.971178, acc.: 4.44%] [G loss: 0.135395]\n",
      "217 [D loss: 1.974359, acc.: 4.43%] [G loss: 0.134858]\n",
      "218 [D loss: 1.977456, acc.: 4.43%] [G loss: 0.134320]\n",
      "219 [D loss: 1.980521, acc.: 4.41%] [G loss: 0.133793]\n",
      "220 [D loss: 1.983687, acc.: 4.40%] [G loss: 0.133264]\n",
      "221 [D loss: 1.986777, acc.: 4.39%] [G loss: 0.132742]\n",
      "222 [D loss: 1.989779, acc.: 4.38%] [G loss: 0.132226]\n",
      "223 [D loss: 1.992922, acc.: 4.37%] [G loss: 0.131710]\n",
      "224 [D loss: 1.996003, acc.: 4.36%] [G loss: 0.131203]\n",
      "225 [D loss: 1.999005, acc.: 4.36%] [G loss: 0.130698]\n",
      "226 [D loss: 2.001951, acc.: 4.35%] [G loss: 0.130194]\n",
      "227 [D loss: 2.004956, acc.: 4.34%] [G loss: 0.129693]\n",
      "228 [D loss: 2.007979, acc.: 4.34%] [G loss: 0.129199]\n",
      "229 [D loss: 2.010954, acc.: 4.33%] [G loss: 0.128708]\n",
      "230 [D loss: 2.013941, acc.: 4.33%] [G loss: 0.128218]\n",
      "231 [D loss: 2.016982, acc.: 4.33%] [G loss: 0.127733]\n",
      "232 [D loss: 2.020121, acc.: 4.33%] [G loss: 0.127253]\n",
      "233 [D loss: 2.023179, acc.: 4.32%] [G loss: 0.126780]\n",
      "234 [D loss: 2.026048, acc.: 4.31%] [G loss: 0.126305]\n",
      "235 [D loss: 2.028945, acc.: 4.31%] [G loss: 0.125833]\n",
      "236 [D loss: 2.031856, acc.: 4.31%] [G loss: 0.125368]\n",
      "237 [D loss: 2.034785, acc.: 4.30%] [G loss: 0.124909]\n",
      "238 [D loss: 2.037666, acc.: 4.29%] [G loss: 0.124450]\n",
      "239 [D loss: 2.040564, acc.: 4.28%] [G loss: 0.123993]\n",
      "240 [D loss: 2.043416, acc.: 4.28%] [G loss: 0.123540]\n",
      "241 [D loss: 2.046287, acc.: 4.27%] [G loss: 0.123095]\n",
      "242 [D loss: 2.049151, acc.: 4.26%] [G loss: 0.122653]\n",
      "243 [D loss: 2.051994, acc.: 4.25%] [G loss: 0.122213]\n",
      "244 [D loss: 2.054850, acc.: 4.25%] [G loss: 0.121776]\n",
      "245 [D loss: 2.057569, acc.: 4.25%] [G loss: 0.121341]\n",
      "246 [D loss: 2.060325, acc.: 4.24%] [G loss: 0.120911]\n",
      "247 [D loss: 2.063157, acc.: 4.23%] [G loss: 0.120480]\n",
      "248 [D loss: 2.065991, acc.: 4.22%] [G loss: 0.120056]\n",
      "249 [D loss: 2.068820, acc.: 4.22%] [G loss: 0.119634]\n",
      "250 [D loss: 2.071580, acc.: 4.21%] [G loss: 0.119217]\n",
      "251 [D loss: 2.074310, acc.: 4.21%] [G loss: 0.118802]\n",
      "252 [D loss: 2.077031, acc.: 4.21%] [G loss: 0.118388]\n",
      "253 [D loss: 2.079784, acc.: 4.20%] [G loss: 0.117981]\n",
      "254 [D loss: 2.082579, acc.: 4.19%] [G loss: 0.117570]\n",
      "255 [D loss: 2.085326, acc.: 4.18%] [G loss: 0.117168]\n",
      "256 [D loss: 2.088079, acc.: 4.17%] [G loss: 0.116762]\n",
      "257 [D loss: 2.090752, acc.: 4.17%] [G loss: 0.116365]\n",
      "258 [D loss: 2.093414, acc.: 4.16%] [G loss: 0.115971]\n",
      "259 [D loss: 2.096089, acc.: 4.16%] [G loss: 0.115576]\n",
      "260 [D loss: 2.098714, acc.: 4.15%] [G loss: 0.115186]\n",
      "261 [D loss: 2.101353, acc.: 4.15%] [G loss: 0.114797]\n",
      "262 [D loss: 2.104067, acc.: 4.14%] [G loss: 0.114412]\n",
      "263 [D loss: 2.106781, acc.: 4.14%] [G loss: 0.114028]\n",
      "264 [D loss: 2.109437, acc.: 4.13%] [G loss: 0.113643]\n",
      "265 [D loss: 2.112088, acc.: 4.13%] [G loss: 0.113270]\n",
      "266 [D loss: 2.114730, acc.: 4.12%] [G loss: 0.112895]\n",
      "267 [D loss: 2.117385, acc.: 4.12%] [G loss: 0.112518]\n",
      "268 [D loss: 2.119975, acc.: 4.11%] [G loss: 0.112151]\n",
      "269 [D loss: 2.122505, acc.: 4.11%] [G loss: 0.111782]\n",
      "270 [D loss: 2.125070, acc.: 4.10%] [G loss: 0.111419]\n",
      "271 [D loss: 2.127654, acc.: 4.09%] [G loss: 0.111056]\n",
      "272 [D loss: 2.130187, acc.: 4.08%] [G loss: 0.110696]\n",
      "273 [D loss: 2.132810, acc.: 4.08%] [G loss: 0.110338]\n",
      "274 [D loss: 2.135426, acc.: 4.07%] [G loss: 0.109983]\n",
      "275 [D loss: 2.137936, acc.: 4.07%] [G loss: 0.109628]\n",
      "276 [D loss: 2.140458, acc.: 4.06%] [G loss: 0.109278]\n",
      "277 [D loss: 2.142993, acc.: 4.05%] [G loss: 0.108929]\n",
      "278 [D loss: 2.145530, acc.: 4.05%] [G loss: 0.108582]\n",
      "279 [D loss: 2.148113, acc.: 4.04%] [G loss: 0.108236]\n",
      "280 [D loss: 2.150688, acc.: 4.04%] [G loss: 0.107894]\n",
      "281 [D loss: 2.153216, acc.: 4.04%] [G loss: 0.107556]\n",
      "282 [D loss: 2.155746, acc.: 4.03%] [G loss: 0.107220]\n",
      "283 [D loss: 2.158281, acc.: 4.03%] [G loss: 0.106884]\n",
      "284 [D loss: 2.160798, acc.: 4.02%] [G loss: 0.106551]\n",
      "285 [D loss: 2.163221, acc.: 4.02%] [G loss: 0.106217]\n",
      "286 [D loss: 2.165667, acc.: 4.01%] [G loss: 0.105887]\n",
      "287 [D loss: 2.168129, acc.: 4.01%] [G loss: 0.105558]\n",
      "288 [D loss: 2.170510, acc.: 4.00%] [G loss: 0.105234]\n",
      "289 [D loss: 2.172883, acc.: 4.00%] [G loss: 0.104911]\n",
      "290 [D loss: 2.175258, acc.: 3.99%] [G loss: 0.104592]\n",
      "291 [D loss: 2.177722, acc.: 3.98%] [G loss: 0.104274]\n",
      "292 [D loss: 2.180143, acc.: 3.98%] [G loss: 0.103954]\n",
      "293 [D loss: 2.182541, acc.: 3.98%] [G loss: 0.103637]\n",
      "294 [D loss: 2.184958, acc.: 3.97%] [G loss: 0.103321]\n",
      "295 [D loss: 2.187363, acc.: 3.97%] [G loss: 0.103013]\n",
      "296 [D loss: 2.189712, acc.: 3.96%] [G loss: 0.102702]\n",
      "297 [D loss: 2.192113, acc.: 3.96%] [G loss: 0.102394]\n",
      "298 [D loss: 2.194453, acc.: 3.95%] [G loss: 0.102087]\n",
      "299 [D loss: 2.196792, acc.: 3.95%] [G loss: 0.101781]\n",
      "300 [D loss: 2.199205, acc.: 3.95%] [G loss: 0.101477]\n",
      "301 [D loss: 2.201676, acc.: 3.94%] [G loss: 0.101175]\n",
      "302 [D loss: 2.204088, acc.: 3.94%] [G loss: 0.100877]\n",
      "303 [D loss: 2.206497, acc.: 3.93%] [G loss: 0.100580]\n",
      "304 [D loss: 2.208898, acc.: 3.93%] [G loss: 0.100286]\n",
      "305 [D loss: 2.211267, acc.: 3.93%] [G loss: 0.099992]\n",
      "306 [D loss: 2.213663, acc.: 3.93%] [G loss: 0.099697]\n",
      "307 [D loss: 2.216078, acc.: 3.92%] [G loss: 0.099406]\n",
      "308 [D loss: 2.218398, acc.: 3.92%] [G loss: 0.099119]\n",
      "309 [D loss: 2.220656, acc.: 3.91%] [G loss: 0.098831]\n",
      "310 [D loss: 2.222970, acc.: 3.91%] [G loss: 0.098546]\n",
      "311 [D loss: 2.225307, acc.: 3.91%] [G loss: 0.098263]\n",
      "312 [D loss: 2.227643, acc.: 3.90%] [G loss: 0.097982]\n",
      "313 [D loss: 2.229926, acc.: 3.90%] [G loss: 0.097701]\n",
      "314 [D loss: 2.232070, acc.: 3.90%] [G loss: 0.097420]\n",
      "315 [D loss: 2.234303, acc.: 3.89%] [G loss: 0.097144]\n",
      "316 [D loss: 2.236595, acc.: 3.89%] [G loss: 0.096870]\n",
      "317 [D loss: 2.238835, acc.: 3.89%] [G loss: 0.096594]\n",
      "318 [D loss: 2.241067, acc.: 3.88%] [G loss: 0.096321]\n",
      "319 [D loss: 2.243380, acc.: 3.88%] [G loss: 0.096050]\n",
      "320 [D loss: 2.245631, acc.: 3.88%] [G loss: 0.095784]\n",
      "321 [D loss: 2.247858, acc.: 3.88%] [G loss: 0.095514]\n",
      "322 [D loss: 2.250097, acc.: 3.88%] [G loss: 0.095248]\n",
      "323 [D loss: 2.252288, acc.: 3.87%] [G loss: 0.094985]\n",
      "324 [D loss: 2.254438, acc.: 3.87%] [G loss: 0.094721]\n",
      "325 [D loss: 2.256682, acc.: 3.86%] [G loss: 0.094459]\n",
      "326 [D loss: 2.258900, acc.: 3.86%] [G loss: 0.094200]\n",
      "327 [D loss: 2.261152, acc.: 3.86%] [G loss: 0.093943]\n",
      "328 [D loss: 2.263337, acc.: 3.86%] [G loss: 0.093686]\n",
      "329 [D loss: 2.265509, acc.: 3.85%] [G loss: 0.093431]\n",
      "330 [D loss: 2.267631, acc.: 3.85%] [G loss: 0.093176]\n",
      "331 [D loss: 2.269766, acc.: 3.85%] [G loss: 0.092924]\n",
      "332 [D loss: 2.271834, acc.: 3.84%] [G loss: 0.092673]\n",
      "333 [D loss: 2.274005, acc.: 3.84%] [G loss: 0.092423]\n",
      "334 [D loss: 2.276094, acc.: 3.84%] [G loss: 0.092173]\n",
      "335 [D loss: 2.278244, acc.: 3.84%] [G loss: 0.091927]\n",
      "336 [D loss: 2.280437, acc.: 3.83%] [G loss: 0.091680]\n",
      "337 [D loss: 2.282654, acc.: 3.83%] [G loss: 0.091434]\n",
      "338 [D loss: 2.284819, acc.: 3.82%] [G loss: 0.091192]\n",
      "339 [D loss: 2.286910, acc.: 3.82%] [G loss: 0.090949]\n",
      "340 [D loss: 2.288989, acc.: 3.81%] [G loss: 0.090708]\n",
      "341 [D loss: 2.291067, acc.: 3.81%] [G loss: 0.090468]\n",
      "342 [D loss: 2.293148, acc.: 3.81%] [G loss: 0.090230]\n",
      "343 [D loss: 2.295227, acc.: 3.81%] [G loss: 0.089995]\n",
      "344 [D loss: 2.297321, acc.: 3.80%] [G loss: 0.089759]\n",
      "345 [D loss: 2.299435, acc.: 3.80%] [G loss: 0.089525]\n",
      "346 [D loss: 2.301562, acc.: 3.80%] [G loss: 0.089291]\n",
      "347 [D loss: 2.303634, acc.: 3.80%] [G loss: 0.089062]\n",
      "348 [D loss: 2.305625, acc.: 3.80%] [G loss: 0.088832]\n",
      "349 [D loss: 2.307642, acc.: 3.79%] [G loss: 0.088605]\n",
      "350 [D loss: 2.309688, acc.: 3.79%] [G loss: 0.088374]\n",
      "351 [D loss: 2.311691, acc.: 3.79%] [G loss: 0.088147]\n",
      "352 [D loss: 2.313686, acc.: 3.79%] [G loss: 0.087919]\n",
      "353 [D loss: 2.315735, acc.: 3.79%] [G loss: 0.087694]\n",
      "354 [D loss: 2.317783, acc.: 3.78%] [G loss: 0.087469]\n",
      "355 [D loss: 2.319849, acc.: 3.78%] [G loss: 0.087247]\n",
      "356 [D loss: 2.321892, acc.: 3.78%] [G loss: 0.087025]\n",
      "357 [D loss: 2.323887, acc.: 3.78%] [G loss: 0.086805]\n",
      "358 [D loss: 2.325825, acc.: 3.78%] [G loss: 0.086586]\n",
      "359 [D loss: 2.327785, acc.: 3.78%] [G loss: 0.086368]\n",
      "360 [D loss: 2.329851, acc.: 3.78%] [G loss: 0.086151]\n",
      "361 [D loss: 2.331850, acc.: 3.78%] [G loss: 0.085934]\n",
      "362 [D loss: 2.333822, acc.: 3.77%] [G loss: 0.085721]\n",
      "363 [D loss: 2.335769, acc.: 3.77%] [G loss: 0.085507]\n",
      "364 [D loss: 2.337767, acc.: 3.77%] [G loss: 0.085294]\n",
      "365 [D loss: 2.339745, acc.: 3.77%] [G loss: 0.085083]\n",
      "366 [D loss: 2.341599, acc.: 3.77%] [G loss: 0.084872]\n",
      "367 [D loss: 2.343498, acc.: 3.76%] [G loss: 0.084662]\n",
      "368 [D loss: 2.345460, acc.: 3.76%] [G loss: 0.084454]\n",
      "369 [D loss: 2.347406, acc.: 3.75%] [G loss: 0.084247]\n",
      "370 [D loss: 2.349362, acc.: 3.75%] [G loss: 0.084043]\n",
      "371 [D loss: 2.351303, acc.: 3.75%] [G loss: 0.083840]\n",
      "372 [D loss: 2.353161, acc.: 3.75%] [G loss: 0.083635]\n",
      "373 [D loss: 2.354999, acc.: 3.75%] [G loss: 0.083433]\n",
      "374 [D loss: 2.356885, acc.: 3.75%] [G loss: 0.083230]\n",
      "375 [D loss: 2.358845, acc.: 3.75%] [G loss: 0.083029]\n",
      "376 [D loss: 2.360733, acc.: 3.74%] [G loss: 0.082829]\n",
      "377 [D loss: 2.362622, acc.: 3.74%] [G loss: 0.082630]\n",
      "378 [D loss: 2.364500, acc.: 3.74%] [G loss: 0.082432]\n",
      "379 [D loss: 2.366354, acc.: 3.73%] [G loss: 0.082235]\n",
      "380 [D loss: 2.368256, acc.: 3.73%] [G loss: 0.082038]\n",
      "381 [D loss: 2.370171, acc.: 3.73%] [G loss: 0.081842]\n",
      "382 [D loss: 2.372100, acc.: 3.72%] [G loss: 0.081648]\n",
      "383 [D loss: 2.373957, acc.: 3.72%] [G loss: 0.081453]\n",
      "384 [D loss: 2.375811, acc.: 3.72%] [G loss: 0.081261]\n",
      "385 [D loss: 2.377663, acc.: 3.71%] [G loss: 0.081069]\n",
      "386 [D loss: 2.379535, acc.: 3.71%] [G loss: 0.080879]\n",
      "387 [D loss: 2.381406, acc.: 3.71%] [G loss: 0.080688]\n",
      "388 [D loss: 2.383224, acc.: 3.71%] [G loss: 0.080499]\n",
      "389 [D loss: 2.385048, acc.: 3.71%] [G loss: 0.080311]\n",
      "390 [D loss: 2.386948, acc.: 3.71%] [G loss: 0.080124]\n",
      "391 [D loss: 2.388802, acc.: 3.70%] [G loss: 0.079937]\n",
      "392 [D loss: 2.390589, acc.: 3.70%] [G loss: 0.079750]\n",
      "393 [D loss: 2.392366, acc.: 3.70%] [G loss: 0.079566]\n",
      "394 [D loss: 2.394180, acc.: 3.70%] [G loss: 0.079383]\n",
      "395 [D loss: 2.395959, acc.: 3.70%] [G loss: 0.079200]\n",
      "396 [D loss: 2.397730, acc.: 3.70%] [G loss: 0.079019]\n",
      "397 [D loss: 2.399504, acc.: 3.69%] [G loss: 0.078839]\n",
      "398 [D loss: 2.401278, acc.: 3.69%] [G loss: 0.078658]\n",
      "399 [D loss: 2.403110, acc.: 3.69%] [G loss: 0.078479]\n",
      "400 [D loss: 2.404892, acc.: 3.69%] [G loss: 0.078301]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "401 [D loss: 2.406667, acc.: 3.69%] [G loss: 0.078123]\n",
      "402 [D loss: 2.408430, acc.: 3.68%] [G loss: 0.077946]\n",
      "403 [D loss: 2.410179, acc.: 3.68%] [G loss: 0.077770]\n",
      "404 [D loss: 2.411919, acc.: 3.68%] [G loss: 0.077595]\n",
      "405 [D loss: 2.413613, acc.: 3.68%] [G loss: 0.077420]\n",
      "406 [D loss: 2.415389, acc.: 3.68%] [G loss: 0.077246]\n",
      "407 [D loss: 2.417140, acc.: 3.67%] [G loss: 0.077073]\n",
      "408 [D loss: 2.418831, acc.: 3.67%] [G loss: 0.076901]\n",
      "409 [D loss: 2.420548, acc.: 3.67%] [G loss: 0.076730]\n",
      "410 [D loss: 2.422300, acc.: 3.67%] [G loss: 0.076561]\n",
      "411 [D loss: 2.424014, acc.: 3.67%] [G loss: 0.076390]\n",
      "412 [D loss: 2.425696, acc.: 3.67%] [G loss: 0.076222]\n",
      "413 [D loss: 2.427437, acc.: 3.67%] [G loss: 0.076053]\n",
      "414 [D loss: 2.429203, acc.: 3.67%] [G loss: 0.075886]\n",
      "415 [D loss: 2.430985, acc.: 3.66%] [G loss: 0.075719]\n",
      "416 [D loss: 2.432723, acc.: 3.66%] [G loss: 0.075552]\n",
      "417 [D loss: 2.434387, acc.: 3.66%] [G loss: 0.075385]\n",
      "418 [D loss: 2.436127, acc.: 3.66%] [G loss: 0.075221]\n",
      "419 [D loss: 2.437920, acc.: 3.66%] [G loss: 0.075056]\n",
      "420 [D loss: 2.439622, acc.: 3.66%] [G loss: 0.074893]\n",
      "421 [D loss: 2.441342, acc.: 3.65%] [G loss: 0.074730]\n",
      "422 [D loss: 2.443068, acc.: 3.65%] [G loss: 0.074568]\n",
      "423 [D loss: 2.444814, acc.: 3.65%] [G loss: 0.074407]\n",
      "424 [D loss: 2.446477, acc.: 3.64%] [G loss: 0.074247]\n",
      "425 [D loss: 2.448151, acc.: 3.64%] [G loss: 0.074087]\n",
      "426 [D loss: 2.449774, acc.: 3.64%] [G loss: 0.073928]\n",
      "427 [D loss: 2.451403, acc.: 3.64%] [G loss: 0.073771]\n",
      "428 [D loss: 2.452996, acc.: 3.64%] [G loss: 0.073613]\n",
      "429 [D loss: 2.454602, acc.: 3.64%] [G loss: 0.073456]\n",
      "430 [D loss: 2.456254, acc.: 3.63%] [G loss: 0.073299]\n",
      "431 [D loss: 2.457944, acc.: 3.63%] [G loss: 0.073143]\n",
      "432 [D loss: 2.459598, acc.: 3.63%] [G loss: 0.072989]\n",
      "433 [D loss: 2.461272, acc.: 3.63%] [G loss: 0.072835]\n",
      "434 [D loss: 2.462959, acc.: 3.63%] [G loss: 0.072680]\n",
      "435 [D loss: 2.464593, acc.: 3.63%] [G loss: 0.072527]\n",
      "436 [D loss: 2.466205, acc.: 3.63%] [G loss: 0.072376]\n",
      "437 [D loss: 2.467777, acc.: 3.62%] [G loss: 0.072224]\n",
      "438 [D loss: 2.469379, acc.: 3.62%] [G loss: 0.072075]\n",
      "439 [D loss: 2.470939, acc.: 3.62%] [G loss: 0.071924]\n",
      "440 [D loss: 2.472530, acc.: 3.62%] [G loss: 0.071774]\n",
      "441 [D loss: 2.474109, acc.: 3.62%] [G loss: 0.071625]\n",
      "442 [D loss: 2.475749, acc.: 3.62%] [G loss: 0.071477]\n",
      "443 [D loss: 2.477411, acc.: 3.61%] [G loss: 0.071330]\n",
      "444 [D loss: 2.479035, acc.: 3.61%] [G loss: 0.071183]\n",
      "445 [D loss: 2.480678, acc.: 3.61%] [G loss: 0.071037]\n",
      "446 [D loss: 2.482249, acc.: 3.61%] [G loss: 0.070891]\n",
      "447 [D loss: 2.483817, acc.: 3.61%] [G loss: 0.070746]\n",
      "448 [D loss: 2.485479, acc.: 3.60%] [G loss: 0.070603]\n",
      "449 [D loss: 2.487118, acc.: 3.60%] [G loss: 0.070458]\n",
      "450 [D loss: 2.488730, acc.: 3.60%] [G loss: 0.070314]\n",
      "451 [D loss: 2.490332, acc.: 3.60%] [G loss: 0.070171]\n",
      "452 [D loss: 2.491932, acc.: 3.60%] [G loss: 0.070029]\n",
      "453 [D loss: 2.493516, acc.: 3.59%] [G loss: 0.069887]\n",
      "454 [D loss: 2.495104, acc.: 3.59%] [G loss: 0.069746]\n",
      "455 [D loss: 2.496682, acc.: 3.59%] [G loss: 0.069605]\n",
      "456 [D loss: 2.498294, acc.: 3.59%] [G loss: 0.069465]\n",
      "457 [D loss: 2.499888, acc.: 3.59%] [G loss: 0.069325]\n",
      "458 [D loss: 2.501481, acc.: 3.59%] [G loss: 0.069186]\n",
      "459 [D loss: 2.503027, acc.: 3.59%] [G loss: 0.069048]\n",
      "460 [D loss: 2.504592, acc.: 3.59%] [G loss: 0.068911]\n",
      "461 [D loss: 2.506104, acc.: 3.58%] [G loss: 0.068773]\n",
      "462 [D loss: 2.507676, acc.: 3.58%] [G loss: 0.068637]\n",
      "463 [D loss: 2.509250, acc.: 3.58%] [G loss: 0.068501]\n",
      "464 [D loss: 2.510756, acc.: 3.58%] [G loss: 0.068365]\n",
      "465 [D loss: 2.512244, acc.: 3.58%] [G loss: 0.068230]\n",
      "466 [D loss: 2.513825, acc.: 3.58%] [G loss: 0.068096]\n",
      "467 [D loss: 2.515418, acc.: 3.58%] [G loss: 0.067963]\n",
      "468 [D loss: 2.516980, acc.: 3.58%] [G loss: 0.067830]\n",
      "469 [D loss: 2.518534, acc.: 3.58%] [G loss: 0.067698]\n",
      "470 [D loss: 2.520088, acc.: 3.57%] [G loss: 0.067565]\n",
      "471 [D loss: 2.521637, acc.: 3.57%] [G loss: 0.067433]\n",
      "472 [D loss: 2.523221, acc.: 3.57%] [G loss: 0.067302]\n",
      "473 [D loss: 2.524760, acc.: 3.57%] [G loss: 0.067172]\n",
      "474 [D loss: 2.526291, acc.: 3.56%] [G loss: 0.067041]\n",
      "475 [D loss: 2.527779, acc.: 3.56%] [G loss: 0.066911]\n",
      "476 [D loss: 2.529266, acc.: 3.56%] [G loss: 0.066782]\n",
      "477 [D loss: 2.530701, acc.: 3.56%] [G loss: 0.066654]\n",
      "478 [D loss: 2.532163, acc.: 3.56%] [G loss: 0.066526]\n",
      "479 [D loss: 2.533629, acc.: 3.55%] [G loss: 0.066398]\n",
      "480 [D loss: 2.535109, acc.: 3.55%] [G loss: 0.066272]\n",
      "481 [D loss: 2.536583, acc.: 3.55%] [G loss: 0.066145]\n",
      "482 [D loss: 2.538086, acc.: 3.55%] [G loss: 0.066020]\n",
      "483 [D loss: 2.539621, acc.: 3.55%] [G loss: 0.065893]\n",
      "484 [D loss: 2.541135, acc.: 3.55%] [G loss: 0.065767]\n",
      "485 [D loss: 2.542675, acc.: 3.55%] [G loss: 0.065643]\n",
      "486 [D loss: 2.544143, acc.: 3.55%] [G loss: 0.065519]\n",
      "487 [D loss: 2.545668, acc.: 3.54%] [G loss: 0.065394]\n",
      "488 [D loss: 2.547192, acc.: 3.54%] [G loss: 0.065271]\n",
      "489 [D loss: 2.548707, acc.: 3.54%] [G loss: 0.065148]\n",
      "490 [D loss: 2.550144, acc.: 3.54%] [G loss: 0.065026]\n",
      "491 [D loss: 2.551623, acc.: 3.54%] [G loss: 0.064905]\n",
      "492 [D loss: 2.553112, acc.: 3.54%] [G loss: 0.064783]\n",
      "493 [D loss: 2.554564, acc.: 3.54%] [G loss: 0.064662]\n",
      "494 [D loss: 2.556065, acc.: 3.54%] [G loss: 0.064542]\n",
      "495 [D loss: 2.557533, acc.: 3.54%] [G loss: 0.064423]\n",
      "496 [D loss: 2.558994, acc.: 3.53%] [G loss: 0.064305]\n",
      "497 [D loss: 2.560447, acc.: 3.53%] [G loss: 0.064186]\n",
      "498 [D loss: 2.561857, acc.: 3.53%] [G loss: 0.064067]\n",
      "499 [D loss: 2.563293, acc.: 3.53%] [G loss: 0.063948]\n",
      "500 [D loss: 2.564740, acc.: 3.53%] [G loss: 0.063831]\n",
      "501 [D loss: 2.566194, acc.: 3.53%] [G loss: 0.063713]\n",
      "502 [D loss: 2.567615, acc.: 3.53%] [G loss: 0.063596]\n",
      "503 [D loss: 2.568996, acc.: 3.52%] [G loss: 0.063480]\n",
      "504 [D loss: 2.570418, acc.: 3.52%] [G loss: 0.063364]\n",
      "505 [D loss: 2.571862, acc.: 3.52%] [G loss: 0.063248]\n",
      "506 [D loss: 2.573261, acc.: 3.52%] [G loss: 0.063133]\n",
      "507 [D loss: 2.574671, acc.: 3.52%] [G loss: 0.063018]\n",
      "508 [D loss: 2.576091, acc.: 3.52%] [G loss: 0.062903]\n",
      "509 [D loss: 2.577553, acc.: 3.52%] [G loss: 0.062789]\n",
      "510 [D loss: 2.578936, acc.: 3.52%] [G loss: 0.062676]\n",
      "511 [D loss: 2.580364, acc.: 3.52%] [G loss: 0.062562]\n",
      "512 [D loss: 2.581769, acc.: 3.52%] [G loss: 0.062450]\n",
      "513 [D loss: 2.583149, acc.: 3.51%] [G loss: 0.062337]\n",
      "514 [D loss: 2.584537, acc.: 3.51%] [G loss: 0.062225]\n",
      "515 [D loss: 2.585952, acc.: 3.51%] [G loss: 0.062113]\n",
      "516 [D loss: 2.587344, acc.: 3.51%] [G loss: 0.062002]\n",
      "517 [D loss: 2.588726, acc.: 3.51%] [G loss: 0.061891]\n",
      "518 [D loss: 2.590106, acc.: 3.51%] [G loss: 0.061781]\n",
      "519 [D loss: 2.591446, acc.: 3.50%] [G loss: 0.061672]\n",
      "520 [D loss: 2.592854, acc.: 3.50%] [G loss: 0.061562]\n",
      "521 [D loss: 2.594212, acc.: 3.50%] [G loss: 0.061453]\n",
      "522 [D loss: 2.595567, acc.: 3.50%] [G loss: 0.061344]\n",
      "523 [D loss: 2.596975, acc.: 3.50%] [G loss: 0.061236]\n",
      "524 [D loss: 2.598383, acc.: 3.49%] [G loss: 0.061128]\n",
      "525 [D loss: 2.599790, acc.: 3.49%] [G loss: 0.061020]\n",
      "526 [D loss: 2.601189, acc.: 3.49%] [G loss: 0.060914]\n",
      "527 [D loss: 2.602591, acc.: 3.48%] [G loss: 0.060807]\n",
      "528 [D loss: 2.603946, acc.: 3.48%] [G loss: 0.060701]\n",
      "529 [D loss: 2.605293, acc.: 3.48%] [G loss: 0.060595]\n",
      "530 [D loss: 2.606613, acc.: 3.48%] [G loss: 0.060489]\n",
      "531 [D loss: 2.607950, acc.: 3.48%] [G loss: 0.060384]\n",
      "532 [D loss: 2.609308, acc.: 3.48%] [G loss: 0.060279]\n",
      "533 [D loss: 2.610663, acc.: 3.48%] [G loss: 0.060174]\n",
      "534 [D loss: 2.612041, acc.: 3.48%] [G loss: 0.060070]\n",
      "535 [D loss: 2.613358, acc.: 3.48%] [G loss: 0.059966]\n",
      "536 [D loss: 2.614701, acc.: 3.48%] [G loss: 0.059862]\n",
      "537 [D loss: 2.616071, acc.: 3.48%] [G loss: 0.059759]\n",
      "538 [D loss: 2.617452, acc.: 3.48%] [G loss: 0.059656]\n",
      "539 [D loss: 2.618791, acc.: 3.47%] [G loss: 0.059554]\n",
      "540 [D loss: 2.620168, acc.: 3.47%] [G loss: 0.059452]\n",
      "541 [D loss: 2.621496, acc.: 3.47%] [G loss: 0.059350]\n",
      "542 [D loss: 2.622817, acc.: 3.47%] [G loss: 0.059249]\n",
      "543 [D loss: 2.624146, acc.: 3.47%] [G loss: 0.059147]\n",
      "544 [D loss: 2.625519, acc.: 3.47%] [G loss: 0.059046]\n",
      "545 [D loss: 2.626855, acc.: 3.47%] [G loss: 0.058946]\n",
      "546 [D loss: 2.628224, acc.: 3.46%] [G loss: 0.058846]\n",
      "547 [D loss: 2.629563, acc.: 3.46%] [G loss: 0.058746]\n",
      "548 [D loss: 2.630849, acc.: 3.46%] [G loss: 0.058648]\n",
      "549 [D loss: 2.632131, acc.: 3.46%] [G loss: 0.058548]\n",
      "550 [D loss: 2.633467, acc.: 3.46%] [G loss: 0.058450]\n",
      "551 [D loss: 2.634808, acc.: 3.46%] [G loss: 0.058351]\n",
      "552 [D loss: 2.636107, acc.: 3.46%] [G loss: 0.058252]\n",
      "553 [D loss: 2.637361, acc.: 3.46%] [G loss: 0.058154]\n",
      "554 [D loss: 2.638639, acc.: 3.46%] [G loss: 0.058057]\n",
      "555 [D loss: 2.639971, acc.: 3.46%] [G loss: 0.057959]\n",
      "556 [D loss: 2.641304, acc.: 3.45%] [G loss: 0.057863]\n",
      "557 [D loss: 2.642622, acc.: 3.45%] [G loss: 0.057767]\n",
      "558 [D loss: 2.643910, acc.: 3.45%] [G loss: 0.057671]\n",
      "559 [D loss: 2.645169, acc.: 3.45%] [G loss: 0.057576]\n",
      "560 [D loss: 2.646439, acc.: 3.45%] [G loss: 0.057480]\n",
      "561 [D loss: 2.647707, acc.: 3.45%] [G loss: 0.057386]\n",
      "562 [D loss: 2.649008, acc.: 3.44%] [G loss: 0.057291]\n",
      "563 [D loss: 2.650230, acc.: 3.45%] [G loss: 0.057197]\n",
      "564 [D loss: 2.651497, acc.: 3.44%] [G loss: 0.057102]\n",
      "565 [D loss: 2.652797, acc.: 3.44%] [G loss: 0.057009]\n",
      "566 [D loss: 2.654075, acc.: 3.44%] [G loss: 0.056915]\n",
      "567 [D loss: 2.655342, acc.: 3.44%] [G loss: 0.056822]\n",
      "568 [D loss: 2.656576, acc.: 3.44%] [G loss: 0.056730]\n",
      "569 [D loss: 2.657852, acc.: 3.44%] [G loss: 0.056638]\n",
      "570 [D loss: 2.659103, acc.: 3.44%] [G loss: 0.056546]\n",
      "571 [D loss: 2.660346, acc.: 3.44%] [G loss: 0.056453]\n",
      "572 [D loss: 2.661620, acc.: 3.43%] [G loss: 0.056362]\n",
      "573 [D loss: 2.662892, acc.: 3.43%] [G loss: 0.056271]\n",
      "574 [D loss: 2.664149, acc.: 3.43%] [G loss: 0.056179]\n",
      "575 [D loss: 2.665401, acc.: 3.43%] [G loss: 0.056088]\n",
      "576 [D loss: 2.666604, acc.: 3.43%] [G loss: 0.055998]\n",
      "577 [D loss: 2.667847, acc.: 3.43%] [G loss: 0.055907]\n",
      "578 [D loss: 2.669087, acc.: 3.43%] [G loss: 0.055818]\n",
      "579 [D loss: 2.670402, acc.: 3.43%] [G loss: 0.055728]\n",
      "580 [D loss: 2.671696, acc.: 3.43%] [G loss: 0.055639]\n",
      "581 [D loss: 2.672947, acc.: 3.43%] [G loss: 0.055550]\n",
      "582 [D loss: 2.674181, acc.: 3.43%] [G loss: 0.055461]\n",
      "583 [D loss: 2.675431, acc.: 3.43%] [G loss: 0.055373]\n",
      "584 [D loss: 2.676701, acc.: 3.43%] [G loss: 0.055285]\n",
      "585 [D loss: 2.677952, acc.: 3.42%] [G loss: 0.055196]\n",
      "586 [D loss: 2.679205, acc.: 3.42%] [G loss: 0.055109]\n",
      "587 [D loss: 2.680419, acc.: 3.42%] [G loss: 0.055022]\n",
      "588 [D loss: 2.681664, acc.: 3.42%] [G loss: 0.054935]\n",
      "589 [D loss: 2.682893, acc.: 3.42%] [G loss: 0.054848]\n",
      "590 [D loss: 2.684162, acc.: 3.42%] [G loss: 0.054761]\n",
      "591 [D loss: 2.685407, acc.: 3.42%] [G loss: 0.054675]\n",
      "592 [D loss: 2.686647, acc.: 3.42%] [G loss: 0.054589]\n",
      "593 [D loss: 2.687873, acc.: 3.42%] [G loss: 0.054504]\n",
      "594 [D loss: 2.689066, acc.: 3.42%] [G loss: 0.054419]\n",
      "595 [D loss: 2.690260, acc.: 3.42%] [G loss: 0.054334]\n",
      "596 [D loss: 2.691471, acc.: 3.42%] [G loss: 0.054249]\n",
      "597 [D loss: 2.692662, acc.: 3.42%] [G loss: 0.054165]\n",
      "598 [D loss: 2.693879, acc.: 3.41%] [G loss: 0.054081]\n",
      "599 [D loss: 2.695082, acc.: 3.41%] [G loss: 0.053997]\n",
      "600 [D loss: 2.696254, acc.: 3.41%] [G loss: 0.053913]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "601 [D loss: 2.697451, acc.: 3.41%] [G loss: 0.053830]\n",
      "602 [D loss: 2.698677, acc.: 3.41%] [G loss: 0.053746]\n",
      "603 [D loss: 2.699878, acc.: 3.41%] [G loss: 0.053664]\n",
      "604 [D loss: 2.701071, acc.: 3.41%] [G loss: 0.053581]\n",
      "605 [D loss: 2.702295, acc.: 3.40%] [G loss: 0.053498]\n",
      "606 [D loss: 2.703498, acc.: 3.40%] [G loss: 0.053416]\n",
      "607 [D loss: 2.704705, acc.: 3.40%] [G loss: 0.053334]\n",
      "608 [D loss: 2.705915, acc.: 3.40%] [G loss: 0.053252]\n",
      "609 [D loss: 2.707119, acc.: 3.40%] [G loss: 0.053171]\n",
      "610 [D loss: 2.708300, acc.: 3.40%] [G loss: 0.053090]\n",
      "611 [D loss: 2.709475, acc.: 3.40%] [G loss: 0.053008]\n",
      "612 [D loss: 2.710667, acc.: 3.40%] [G loss: 0.052928]\n",
      "613 [D loss: 2.711861, acc.: 3.40%] [G loss: 0.052848]\n",
      "614 [D loss: 2.713026, acc.: 3.39%] [G loss: 0.052767]\n",
      "615 [D loss: 2.714184, acc.: 3.39%] [G loss: 0.052688]\n",
      "616 [D loss: 2.715338, acc.: 3.39%] [G loss: 0.052608]\n",
      "617 [D loss: 2.716541, acc.: 3.39%] [G loss: 0.052529]\n",
      "618 [D loss: 2.717709, acc.: 3.39%] [G loss: 0.052450]\n",
      "619 [D loss: 2.718845, acc.: 3.39%] [G loss: 0.052371]\n",
      "620 [D loss: 2.719963, acc.: 3.39%] [G loss: 0.052292]\n",
      "621 [D loss: 2.721122, acc.: 3.39%] [G loss: 0.052214]\n",
      "622 [D loss: 2.722294, acc.: 3.39%] [G loss: 0.052136]\n",
      "623 [D loss: 2.723474, acc.: 3.39%] [G loss: 0.052058]\n",
      "624 [D loss: 2.724618, acc.: 3.38%] [G loss: 0.051981]\n",
      "625 [D loss: 2.725752, acc.: 3.38%] [G loss: 0.051903]\n",
      "626 [D loss: 2.726850, acc.: 3.38%] [G loss: 0.051826]\n",
      "627 [D loss: 2.728001, acc.: 3.38%] [G loss: 0.051749]\n",
      "628 [D loss: 2.729113, acc.: 3.38%] [G loss: 0.051672]\n",
      "629 [D loss: 2.730274, acc.: 3.38%] [G loss: 0.051596]\n",
      "630 [D loss: 2.731451, acc.: 3.38%] [G loss: 0.051519]\n",
      "631 [D loss: 2.732587, acc.: 3.38%] [G loss: 0.051444]\n",
      "632 [D loss: 2.733724, acc.: 3.38%] [G loss: 0.051368]\n",
      "633 [D loss: 2.734885, acc.: 3.38%] [G loss: 0.051292]\n",
      "634 [D loss: 2.736032, acc.: 3.38%] [G loss: 0.051216]\n",
      "635 [D loss: 2.737171, acc.: 3.38%] [G loss: 0.051141]\n",
      "636 [D loss: 2.738303, acc.: 3.37%] [G loss: 0.051066]\n",
      "637 [D loss: 2.739421, acc.: 3.38%] [G loss: 0.050991]\n",
      "638 [D loss: 2.740544, acc.: 3.38%] [G loss: 0.050916]\n",
      "639 [D loss: 2.741687, acc.: 3.37%] [G loss: 0.050841]\n",
      "640 [D loss: 2.742829, acc.: 3.37%] [G loss: 0.050767]\n",
      "641 [D loss: 2.743963, acc.: 3.37%] [G loss: 0.050693]\n",
      "642 [D loss: 2.745106, acc.: 3.37%] [G loss: 0.050620]\n",
      "643 [D loss: 2.746247, acc.: 3.37%] [G loss: 0.050546]\n",
      "644 [D loss: 2.747382, acc.: 3.37%] [G loss: 0.050473]\n",
      "645 [D loss: 2.748492, acc.: 3.37%] [G loss: 0.050400]\n",
      "646 [D loss: 2.749579, acc.: 3.37%] [G loss: 0.050327]\n",
      "647 [D loss: 2.750684, acc.: 3.37%] [G loss: 0.050255]\n",
      "648 [D loss: 2.751792, acc.: 3.37%] [G loss: 0.050182]\n",
      "649 [D loss: 2.752911, acc.: 3.37%] [G loss: 0.050110]\n",
      "650 [D loss: 2.754038, acc.: 3.37%] [G loss: 0.050039]\n",
      "651 [D loss: 2.755141, acc.: 3.37%] [G loss: 0.049967]\n",
      "652 [D loss: 2.756253, acc.: 3.36%] [G loss: 0.049895]\n",
      "653 [D loss: 2.757357, acc.: 3.36%] [G loss: 0.049824]\n",
      "654 [D loss: 2.758437, acc.: 3.36%] [G loss: 0.049753]\n",
      "655 [D loss: 2.759524, acc.: 3.36%] [G loss: 0.049682]\n",
      "656 [D loss: 2.760653, acc.: 3.36%] [G loss: 0.049612]\n",
      "657 [D loss: 2.761796, acc.: 3.36%] [G loss: 0.049542]\n",
      "658 [D loss: 2.762910, acc.: 3.36%] [G loss: 0.049471]\n",
      "659 [D loss: 2.764005, acc.: 3.36%] [G loss: 0.049402]\n",
      "660 [D loss: 2.765060, acc.: 3.36%] [G loss: 0.049332]\n",
      "661 [D loss: 2.766098, acc.: 3.36%] [G loss: 0.049263]\n",
      "662 [D loss: 2.767154, acc.: 3.36%] [G loss: 0.049193]\n",
      "663 [D loss: 2.768217, acc.: 3.36%] [G loss: 0.049124]\n",
      "664 [D loss: 2.769314, acc.: 3.36%] [G loss: 0.049056]\n",
      "665 [D loss: 2.770361, acc.: 3.36%] [G loss: 0.048987]\n",
      "666 [D loss: 2.771418, acc.: 3.36%] [G loss: 0.048918]\n",
      "667 [D loss: 2.772496, acc.: 3.36%] [G loss: 0.048850]\n",
      "668 [D loss: 2.773586, acc.: 3.36%] [G loss: 0.048781]\n",
      "669 [D loss: 2.774688, acc.: 3.36%] [G loss: 0.048713]\n",
      "670 [D loss: 2.775743, acc.: 3.36%] [G loss: 0.048645]\n",
      "671 [D loss: 2.776824, acc.: 3.36%] [G loss: 0.048578]\n",
      "672 [D loss: 2.777914, acc.: 3.36%] [G loss: 0.048510]\n",
      "673 [D loss: 2.779003, acc.: 3.36%] [G loss: 0.048443]\n",
      "674 [D loss: 2.780115, acc.: 3.36%] [G loss: 0.048375]\n",
      "675 [D loss: 2.781187, acc.: 3.36%] [G loss: 0.048309]\n",
      "676 [D loss: 2.782251, acc.: 3.35%] [G loss: 0.048242]\n",
      "677 [D loss: 2.783316, acc.: 3.35%] [G loss: 0.048175]\n",
      "678 [D loss: 2.784386, acc.: 3.35%] [G loss: 0.048109]\n",
      "679 [D loss: 2.785497, acc.: 3.35%] [G loss: 0.048042]\n",
      "680 [D loss: 2.786550, acc.: 3.35%] [G loss: 0.047976]\n",
      "681 [D loss: 2.787622, acc.: 3.35%] [G loss: 0.047910]\n",
      "682 [D loss: 2.788687, acc.: 3.35%] [G loss: 0.047844]\n",
      "683 [D loss: 2.789701, acc.: 3.35%] [G loss: 0.047779]\n",
      "684 [D loss: 2.790755, acc.: 3.35%] [G loss: 0.047714]\n",
      "685 [D loss: 2.791802, acc.: 3.35%] [G loss: 0.047648]\n",
      "686 [D loss: 2.792841, acc.: 3.35%] [G loss: 0.047583]\n",
      "687 [D loss: 2.793875, acc.: 3.35%] [G loss: 0.047518]\n",
      "688 [D loss: 2.794943, acc.: 3.35%] [G loss: 0.047454]\n",
      "689 [D loss: 2.796008, acc.: 3.35%] [G loss: 0.047390]\n",
      "690 [D loss: 2.797088, acc.: 3.35%] [G loss: 0.047325]\n",
      "691 [D loss: 2.798137, acc.: 3.35%] [G loss: 0.047262]\n",
      "692 [D loss: 2.799205, acc.: 3.35%] [G loss: 0.047198]\n",
      "693 [D loss: 2.800258, acc.: 3.34%] [G loss: 0.047134]\n",
      "694 [D loss: 2.801285, acc.: 3.34%] [G loss: 0.047070]\n",
      "695 [D loss: 2.802327, acc.: 3.34%] [G loss: 0.047007]\n",
      "696 [D loss: 2.803389, acc.: 3.34%] [G loss: 0.046943]\n",
      "697 [D loss: 2.804454, acc.: 3.34%] [G loss: 0.046880]\n",
      "698 [D loss: 2.805522, acc.: 3.34%] [G loss: 0.046817]\n",
      "699 [D loss: 2.806553, acc.: 3.34%] [G loss: 0.046755]\n",
      "700 [D loss: 2.807603, acc.: 3.34%] [G loss: 0.046692]\n",
      "701 [D loss: 2.808641, acc.: 3.34%] [G loss: 0.046630]\n",
      "702 [D loss: 2.809686, acc.: 3.34%] [G loss: 0.046567]\n",
      "703 [D loss: 2.810752, acc.: 3.34%] [G loss: 0.046505]\n",
      "704 [D loss: 2.811804, acc.: 3.34%] [G loss: 0.046444]\n",
      "705 [D loss: 2.812880, acc.: 3.33%] [G loss: 0.046382]\n",
      "706 [D loss: 2.813933, acc.: 3.33%] [G loss: 0.046321]\n",
      "707 [D loss: 2.814960, acc.: 3.33%] [G loss: 0.046260]\n",
      "708 [D loss: 2.815986, acc.: 3.33%] [G loss: 0.046198]\n",
      "709 [D loss: 2.817021, acc.: 3.33%] [G loss: 0.046137]\n",
      "710 [D loss: 2.818091, acc.: 3.33%] [G loss: 0.046076]\n",
      "711 [D loss: 2.819086, acc.: 3.33%] [G loss: 0.046015]\n",
      "712 [D loss: 2.820083, acc.: 3.33%] [G loss: 0.045955]\n",
      "713 [D loss: 2.821102, acc.: 3.33%] [G loss: 0.045894]\n",
      "714 [D loss: 2.822127, acc.: 3.33%] [G loss: 0.045834]\n",
      "715 [D loss: 2.823134, acc.: 3.33%] [G loss: 0.045775]\n",
      "716 [D loss: 2.824122, acc.: 3.32%] [G loss: 0.045715]\n",
      "717 [D loss: 2.825115, acc.: 3.32%] [G loss: 0.045655]\n",
      "718 [D loss: 2.826125, acc.: 3.32%] [G loss: 0.045596]\n",
      "719 [D loss: 2.827125, acc.: 3.32%] [G loss: 0.045537]\n",
      "720 [D loss: 2.828136, acc.: 3.32%] [G loss: 0.045477]\n",
      "721 [D loss: 2.829132, acc.: 3.32%] [G loss: 0.045418]\n",
      "722 [D loss: 2.830114, acc.: 3.32%] [G loss: 0.045359]\n",
      "723 [D loss: 2.831090, acc.: 3.32%] [G loss: 0.045301]\n",
      "724 [D loss: 2.832085, acc.: 3.32%] [G loss: 0.045242]\n",
      "725 [D loss: 2.833093, acc.: 3.32%] [G loss: 0.045184]\n",
      "726 [D loss: 2.834086, acc.: 3.32%] [G loss: 0.045125]\n",
      "727 [D loss: 2.835069, acc.: 3.32%] [G loss: 0.045067]\n",
      "728 [D loss: 2.836062, acc.: 3.32%] [G loss: 0.045009]\n",
      "729 [D loss: 2.837040, acc.: 3.32%] [G loss: 0.044951]\n",
      "730 [D loss: 2.838044, acc.: 3.32%] [G loss: 0.044894]\n",
      "731 [D loss: 2.839062, acc.: 3.32%] [G loss: 0.044836]\n",
      "732 [D loss: 2.840051, acc.: 3.32%] [G loss: 0.044779]\n",
      "733 [D loss: 2.841021, acc.: 3.32%] [G loss: 0.044722]\n",
      "734 [D loss: 2.842001, acc.: 3.32%] [G loss: 0.044665]\n",
      "735 [D loss: 2.842991, acc.: 3.32%] [G loss: 0.044607]\n",
      "736 [D loss: 2.843964, acc.: 3.32%] [G loss: 0.044551]\n",
      "737 [D loss: 2.844956, acc.: 3.32%] [G loss: 0.044494]\n",
      "738 [D loss: 2.845934, acc.: 3.32%] [G loss: 0.044437]\n",
      "739 [D loss: 2.846929, acc.: 3.31%] [G loss: 0.044381]\n",
      "740 [D loss: 2.847946, acc.: 3.31%] [G loss: 0.044325]\n",
      "741 [D loss: 2.848939, acc.: 3.31%] [G loss: 0.044269]\n",
      "742 [D loss: 2.849920, acc.: 3.31%] [G loss: 0.044213]\n",
      "743 [D loss: 2.850916, acc.: 3.31%] [G loss: 0.044157]\n",
      "744 [D loss: 2.851903, acc.: 3.31%] [G loss: 0.044101]\n",
      "745 [D loss: 2.852909, acc.: 3.31%] [G loss: 0.044046]\n",
      "746 [D loss: 2.853879, acc.: 3.31%] [G loss: 0.043990]\n",
      "747 [D loss: 2.854841, acc.: 3.31%] [G loss: 0.043935]\n",
      "748 [D loss: 2.855805, acc.: 3.31%] [G loss: 0.043879]\n",
      "749 [D loss: 2.856753, acc.: 3.31%] [G loss: 0.043824]\n",
      "750 [D loss: 2.857740, acc.: 3.31%] [G loss: 0.043769]\n",
      "751 [D loss: 2.858736, acc.: 3.31%] [G loss: 0.043715]\n",
      "752 [D loss: 2.859702, acc.: 3.31%] [G loss: 0.043660]\n",
      "753 [D loss: 2.860640, acc.: 3.31%] [G loss: 0.043606]\n",
      "754 [D loss: 2.861584, acc.: 3.31%] [G loss: 0.043552]\n",
      "755 [D loss: 2.862551, acc.: 3.31%] [G loss: 0.043497]\n",
      "756 [D loss: 2.863521, acc.: 3.31%] [G loss: 0.043443]\n",
      "757 [D loss: 2.864504, acc.: 3.30%] [G loss: 0.043389]\n",
      "758 [D loss: 2.865446, acc.: 3.30%] [G loss: 0.043336]\n",
      "759 [D loss: 2.866395, acc.: 3.30%] [G loss: 0.043282]\n",
      "760 [D loss: 2.867353, acc.: 3.30%] [G loss: 0.043229]\n",
      "761 [D loss: 2.868301, acc.: 3.30%] [G loss: 0.043176]\n",
      "762 [D loss: 2.869240, acc.: 3.30%] [G loss: 0.043122]\n",
      "763 [D loss: 2.870181, acc.: 3.30%] [G loss: 0.043069]\n",
      "764 [D loss: 2.871142, acc.: 3.30%] [G loss: 0.043016]\n",
      "765 [D loss: 2.872113, acc.: 3.30%] [G loss: 0.042963]\n",
      "766 [D loss: 2.873072, acc.: 3.30%] [G loss: 0.042911]\n",
      "767 [D loss: 2.874001, acc.: 3.30%] [G loss: 0.042858]\n",
      "768 [D loss: 2.874969, acc.: 3.30%] [G loss: 0.042806]\n",
      "769 [D loss: 2.875931, acc.: 3.30%] [G loss: 0.042753]\n",
      "770 [D loss: 2.876869, acc.: 3.30%] [G loss: 0.042701]\n",
      "771 [D loss: 2.877819, acc.: 3.30%] [G loss: 0.042649]\n",
      "772 [D loss: 2.878749, acc.: 3.30%] [G loss: 0.042597]\n",
      "773 [D loss: 2.879673, acc.: 3.30%] [G loss: 0.042546]\n",
      "774 [D loss: 2.880617, acc.: 3.30%] [G loss: 0.042494]\n",
      "775 [D loss: 2.881556, acc.: 3.30%] [G loss: 0.042442]\n",
      "776 [D loss: 2.882506, acc.: 3.30%] [G loss: 0.042391]\n",
      "777 [D loss: 2.883424, acc.: 3.30%] [G loss: 0.042340]\n",
      "778 [D loss: 2.884306, acc.: 3.30%] [G loss: 0.042289]\n",
      "779 [D loss: 2.885244, acc.: 3.29%] [G loss: 0.042238]\n",
      "780 [D loss: 2.886181, acc.: 3.29%] [G loss: 0.042187]\n",
      "781 [D loss: 2.887081, acc.: 3.29%] [G loss: 0.042136]\n",
      "782 [D loss: 2.887972, acc.: 3.29%] [G loss: 0.042086]\n",
      "783 [D loss: 2.888873, acc.: 3.29%] [G loss: 0.042035]\n",
      "784 [D loss: 2.889815, acc.: 3.29%] [G loss: 0.041985]\n",
      "785 [D loss: 2.890740, acc.: 3.29%] [G loss: 0.041935]\n",
      "786 [D loss: 2.891688, acc.: 3.29%] [G loss: 0.041884]\n",
      "787 [D loss: 2.892622, acc.: 3.29%] [G loss: 0.041834]\n",
      "788 [D loss: 2.893528, acc.: 3.29%] [G loss: 0.041785]\n",
      "789 [D loss: 2.894453, acc.: 3.29%] [G loss: 0.041735]\n",
      "790 [D loss: 2.895381, acc.: 3.29%] [G loss: 0.041685]\n",
      "791 [D loss: 2.896301, acc.: 3.29%] [G loss: 0.041636]\n",
      "792 [D loss: 2.897217, acc.: 3.29%] [G loss: 0.041586]\n",
      "793 [D loss: 2.898134, acc.: 3.29%] [G loss: 0.041537]\n",
      "794 [D loss: 2.899038, acc.: 3.29%] [G loss: 0.041488]\n",
      "795 [D loss: 2.899938, acc.: 3.29%] [G loss: 0.041439]\n",
      "796 [D loss: 2.900829, acc.: 3.29%] [G loss: 0.041390]\n",
      "797 [D loss: 2.901731, acc.: 3.29%] [G loss: 0.041341]\n",
      "798 [D loss: 2.902620, acc.: 3.29%] [G loss: 0.041292]\n",
      "799 [D loss: 2.903526, acc.: 3.28%] [G loss: 0.041243]\n",
      "800 [D loss: 2.904422, acc.: 3.28%] [G loss: 0.041195]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "801 [D loss: 2.905332, acc.: 3.28%] [G loss: 0.041147]\n",
      "802 [D loss: 2.906198, acc.: 3.28%] [G loss: 0.041098]\n",
      "803 [D loss: 2.907085, acc.: 3.28%] [G loss: 0.041050]\n",
      "804 [D loss: 2.908007, acc.: 3.28%] [G loss: 0.041002]\n",
      "805 [D loss: 2.908918, acc.: 3.28%] [G loss: 0.040954]\n",
      "806 [D loss: 2.909835, acc.: 3.28%] [G loss: 0.040906]\n",
      "807 [D loss: 2.910729, acc.: 3.28%] [G loss: 0.040859]\n",
      "808 [D loss: 2.911621, acc.: 3.28%] [G loss: 0.040811]\n",
      "809 [D loss: 2.912521, acc.: 3.28%] [G loss: 0.040764]\n",
      "810 [D loss: 2.913452, acc.: 3.28%] [G loss: 0.040717]\n",
      "811 [D loss: 2.914361, acc.: 3.28%] [G loss: 0.040669]\n",
      "812 [D loss: 2.915285, acc.: 3.28%] [G loss: 0.040622]\n",
      "813 [D loss: 2.916168, acc.: 3.28%] [G loss: 0.040575]\n",
      "814 [D loss: 2.917046, acc.: 3.28%] [G loss: 0.040528]\n",
      "815 [D loss: 2.917920, acc.: 3.28%] [G loss: 0.040482]\n",
      "816 [D loss: 2.918786, acc.: 3.28%] [G loss: 0.040435]\n",
      "817 [D loss: 2.919666, acc.: 3.28%] [G loss: 0.040388]\n",
      "818 [D loss: 2.920547, acc.: 3.28%] [G loss: 0.040342]\n",
      "819 [D loss: 2.921437, acc.: 3.27%] [G loss: 0.040296]\n",
      "820 [D loss: 2.922324, acc.: 3.27%] [G loss: 0.040249]\n",
      "821 [D loss: 2.923228, acc.: 3.27%] [G loss: 0.040203]\n",
      "822 [D loss: 2.924140, acc.: 3.27%] [G loss: 0.040157]\n",
      "823 [D loss: 2.925028, acc.: 3.27%] [G loss: 0.040111]\n",
      "824 [D loss: 2.925877, acc.: 3.27%] [G loss: 0.040065]\n",
      "825 [D loss: 2.926743, acc.: 3.27%] [G loss: 0.040019]\n",
      "826 [D loss: 2.927623, acc.: 3.27%] [G loss: 0.039973]\n",
      "827 [D loss: 2.928489, acc.: 3.27%] [G loss: 0.039928]\n",
      "828 [D loss: 2.929377, acc.: 3.27%] [G loss: 0.039883]\n",
      "829 [D loss: 2.930253, acc.: 3.27%] [G loss: 0.039837]\n",
      "830 [D loss: 2.931138, acc.: 3.27%] [G loss: 0.039792]\n",
      "831 [D loss: 2.932031, acc.: 3.27%] [G loss: 0.039747]\n",
      "832 [D loss: 2.932905, acc.: 3.27%] [G loss: 0.039702]\n",
      "833 [D loss: 2.933787, acc.: 3.27%] [G loss: 0.039657]\n",
      "834 [D loss: 2.934660, acc.: 3.27%] [G loss: 0.039612]\n",
      "835 [D loss: 2.935508, acc.: 3.27%] [G loss: 0.039568]\n",
      "836 [D loss: 2.936384, acc.: 3.27%] [G loss: 0.039523]\n",
      "837 [D loss: 2.937241, acc.: 3.27%] [G loss: 0.039479]\n",
      "838 [D loss: 2.938105, acc.: 3.27%] [G loss: 0.039434]\n",
      "839 [D loss: 2.938985, acc.: 3.27%] [G loss: 0.039390]\n",
      "840 [D loss: 2.939852, acc.: 3.27%] [G loss: 0.039346]\n",
      "841 [D loss: 2.940717, acc.: 3.27%] [G loss: 0.039302]\n",
      "842 [D loss: 2.941602, acc.: 3.27%] [G loss: 0.039258]\n",
      "843 [D loss: 2.942463, acc.: 3.27%] [G loss: 0.039214]\n",
      "844 [D loss: 2.943339, acc.: 3.26%] [G loss: 0.039170]\n",
      "845 [D loss: 2.944210, acc.: 3.26%] [G loss: 0.039127]\n",
      "846 [D loss: 2.945091, acc.: 3.26%] [G loss: 0.039083]\n",
      "847 [D loss: 2.945970, acc.: 3.26%] [G loss: 0.039040]\n",
      "848 [D loss: 2.946859, acc.: 3.26%] [G loss: 0.038996]\n",
      "849 [D loss: 2.947733, acc.: 3.26%] [G loss: 0.038953]\n",
      "850 [D loss: 2.948585, acc.: 3.26%] [G loss: 0.038910]\n",
      "851 [D loss: 2.949469, acc.: 3.26%] [G loss: 0.038867]\n",
      "852 [D loss: 2.950325, acc.: 3.26%] [G loss: 0.038824]\n",
      "853 [D loss: 2.951170, acc.: 3.26%] [G loss: 0.038781]\n",
      "854 [D loss: 2.952027, acc.: 3.26%] [G loss: 0.038738]\n",
      "855 [D loss: 2.952862, acc.: 3.26%] [G loss: 0.038696]\n",
      "856 [D loss: 2.953737, acc.: 3.26%] [G loss: 0.038653]\n",
      "857 [D loss: 2.954590, acc.: 3.26%] [G loss: 0.038611]\n",
      "858 [D loss: 2.955413, acc.: 3.26%] [G loss: 0.038568]\n",
      "859 [D loss: 2.956241, acc.: 3.26%] [G loss: 0.038526]\n",
      "860 [D loss: 2.957063, acc.: 3.26%] [G loss: 0.038484]\n",
      "861 [D loss: 2.957899, acc.: 3.26%] [G loss: 0.038442]\n",
      "862 [D loss: 2.958723, acc.: 3.26%] [G loss: 0.038399]\n",
      "863 [D loss: 2.959564, acc.: 3.25%] [G loss: 0.038357]\n",
      "864 [D loss: 2.960425, acc.: 3.25%] [G loss: 0.038315]\n",
      "865 [D loss: 2.961285, acc.: 3.25%] [G loss: 0.038274]\n",
      "866 [D loss: 2.962114, acc.: 3.25%] [G loss: 0.038232]\n",
      "867 [D loss: 2.962937, acc.: 3.25%] [G loss: 0.038191]\n",
      "868 [D loss: 2.963765, acc.: 3.25%] [G loss: 0.038150]\n",
      "869 [D loss: 2.964597, acc.: 3.25%] [G loss: 0.038108]\n",
      "870 [D loss: 2.965420, acc.: 3.25%] [G loss: 0.038067]\n",
      "871 [D loss: 2.966222, acc.: 3.25%] [G loss: 0.038026]\n",
      "872 [D loss: 2.967052, acc.: 3.25%] [G loss: 0.037985]\n",
      "873 [D loss: 2.967898, acc.: 3.25%] [G loss: 0.037944]\n",
      "874 [D loss: 2.968728, acc.: 3.25%] [G loss: 0.037903]\n",
      "875 [D loss: 2.969586, acc.: 3.25%] [G loss: 0.037862]\n",
      "876 [D loss: 2.970437, acc.: 3.25%] [G loss: 0.037821]\n",
      "877 [D loss: 2.971301, acc.: 3.25%] [G loss: 0.037780]\n",
      "878 [D loss: 2.972156, acc.: 3.24%] [G loss: 0.037740]\n",
      "879 [D loss: 2.972986, acc.: 3.24%] [G loss: 0.037699]\n",
      "880 [D loss: 2.973835, acc.: 3.24%] [G loss: 0.037659]\n",
      "881 [D loss: 2.974677, acc.: 3.24%] [G loss: 0.037618]\n",
      "882 [D loss: 2.975508, acc.: 3.24%] [G loss: 0.037578]\n",
      "883 [D loss: 2.976335, acc.: 3.24%] [G loss: 0.037538]\n",
      "884 [D loss: 2.977136, acc.: 3.24%] [G loss: 0.037498]\n",
      "885 [D loss: 2.977949, acc.: 3.24%] [G loss: 0.037458]\n",
      "886 [D loss: 2.978775, acc.: 3.24%] [G loss: 0.037418]\n",
      "887 [D loss: 2.979613, acc.: 3.24%] [G loss: 0.037378]\n",
      "888 [D loss: 2.980430, acc.: 3.24%] [G loss: 0.037339]\n",
      "889 [D loss: 2.981249, acc.: 3.24%] [G loss: 0.037299]\n",
      "890 [D loss: 2.982051, acc.: 3.24%] [G loss: 0.037260]\n",
      "891 [D loss: 2.982861, acc.: 3.24%] [G loss: 0.037220]\n",
      "892 [D loss: 2.983679, acc.: 3.24%] [G loss: 0.037181]\n",
      "893 [D loss: 2.984496, acc.: 3.24%] [G loss: 0.037141]\n",
      "894 [D loss: 2.985308, acc.: 3.24%] [G loss: 0.037102]\n",
      "895 [D loss: 2.986108, acc.: 3.24%] [G loss: 0.037063]\n",
      "896 [D loss: 2.986899, acc.: 3.24%] [G loss: 0.037024]\n",
      "897 [D loss: 2.987713, acc.: 3.24%] [G loss: 0.036985]\n",
      "898 [D loss: 2.988513, acc.: 3.24%] [G loss: 0.036946]\n",
      "899 [D loss: 2.989329, acc.: 3.23%] [G loss: 0.036907]\n",
      "900 [D loss: 2.990131, acc.: 3.23%] [G loss: 0.036868]\n",
      "901 [D loss: 2.990921, acc.: 3.23%] [G loss: 0.036830]\n",
      "902 [D loss: 2.991721, acc.: 3.23%] [G loss: 0.036791]\n",
      "903 [D loss: 2.992525, acc.: 3.23%] [G loss: 0.036753]\n",
      "904 [D loss: 2.993309, acc.: 3.23%] [G loss: 0.036714]\n",
      "905 [D loss: 2.994111, acc.: 3.23%] [G loss: 0.036676]\n",
      "906 [D loss: 2.994917, acc.: 3.23%] [G loss: 0.036638]\n",
      "907 [D loss: 2.995714, acc.: 3.23%] [G loss: 0.036600]\n",
      "908 [D loss: 2.996515, acc.: 3.23%] [G loss: 0.036562]\n",
      "909 [D loss: 2.997310, acc.: 3.23%] [G loss: 0.036524]\n",
      "910 [D loss: 2.998117, acc.: 3.23%] [G loss: 0.036486]\n",
      "911 [D loss: 2.998909, acc.: 3.23%] [G loss: 0.036448]\n",
      "912 [D loss: 2.999720, acc.: 3.23%] [G loss: 0.036410]\n",
      "913 [D loss: 3.000499, acc.: 3.23%] [G loss: 0.036372]\n",
      "914 [D loss: 3.001301, acc.: 3.23%] [G loss: 0.036335]\n",
      "915 [D loss: 3.002086, acc.: 3.23%] [G loss: 0.036297]\n",
      "916 [D loss: 3.002875, acc.: 3.23%] [G loss: 0.036260]\n",
      "917 [D loss: 3.003648, acc.: 3.23%] [G loss: 0.036223]\n",
      "918 [D loss: 3.004421, acc.: 3.23%] [G loss: 0.036185]\n",
      "919 [D loss: 3.005222, acc.: 3.23%] [G loss: 0.036148]\n",
      "920 [D loss: 3.006004, acc.: 3.23%] [G loss: 0.036111]\n",
      "921 [D loss: 3.006780, acc.: 3.23%] [G loss: 0.036074]\n",
      "922 [D loss: 3.007571, acc.: 3.23%] [G loss: 0.036037]\n",
      "923 [D loss: 3.008362, acc.: 3.23%] [G loss: 0.036000]\n",
      "924 [D loss: 3.009139, acc.: 3.23%] [G loss: 0.035964]\n",
      "925 [D loss: 3.009910, acc.: 3.23%] [G loss: 0.035927]\n",
      "926 [D loss: 3.010697, acc.: 3.23%] [G loss: 0.035890]\n",
      "927 [D loss: 3.011508, acc.: 3.23%] [G loss: 0.035853]\n",
      "928 [D loss: 3.012300, acc.: 3.23%] [G loss: 0.035817]\n",
      "929 [D loss: 3.013075, acc.: 3.23%] [G loss: 0.035781]\n",
      "930 [D loss: 3.013831, acc.: 3.23%] [G loss: 0.035744]\n",
      "931 [D loss: 3.014579, acc.: 3.23%] [G loss: 0.035708]\n",
      "932 [D loss: 3.015344, acc.: 3.23%] [G loss: 0.035672]\n",
      "933 [D loss: 3.016102, acc.: 3.23%] [G loss: 0.035635]\n",
      "934 [D loss: 3.016905, acc.: 3.23%] [G loss: 0.035599]\n",
      "935 [D loss: 3.017696, acc.: 3.23%] [G loss: 0.035563]\n",
      "936 [D loss: 3.018459, acc.: 3.23%] [G loss: 0.035528]\n",
      "937 [D loss: 3.019213, acc.: 3.23%] [G loss: 0.035492]\n",
      "938 [D loss: 3.019992, acc.: 3.22%] [G loss: 0.035456]\n",
      "939 [D loss: 3.020756, acc.: 3.22%] [G loss: 0.035420]\n",
      "940 [D loss: 3.021534, acc.: 3.22%] [G loss: 0.035384]\n",
      "941 [D loss: 3.022305, acc.: 3.22%] [G loss: 0.035349]\n",
      "942 [D loss: 3.023076, acc.: 3.22%] [G loss: 0.035313]\n",
      "943 [D loss: 3.023827, acc.: 3.22%] [G loss: 0.035278]\n",
      "944 [D loss: 3.024600, acc.: 3.22%] [G loss: 0.035243]\n",
      "945 [D loss: 3.025392, acc.: 3.22%] [G loss: 0.035207]\n",
      "946 [D loss: 3.026159, acc.: 3.22%] [G loss: 0.035172]\n",
      "947 [D loss: 3.026909, acc.: 3.22%] [G loss: 0.035137]\n",
      "948 [D loss: 3.027661, acc.: 3.22%] [G loss: 0.035102]\n",
      "949 [D loss: 3.028387, acc.: 3.22%] [G loss: 0.035067]\n",
      "950 [D loss: 3.029148, acc.: 3.22%] [G loss: 0.035032]\n",
      "951 [D loss: 3.029881, acc.: 3.22%] [G loss: 0.034997]\n",
      "952 [D loss: 3.030606, acc.: 3.22%] [G loss: 0.034963]\n",
      "953 [D loss: 3.031359, acc.: 3.22%] [G loss: 0.034928]\n",
      "954 [D loss: 3.032118, acc.: 3.22%] [G loss: 0.034893]\n",
      "955 [D loss: 3.032912, acc.: 3.22%] [G loss: 0.034859]\n",
      "956 [D loss: 3.033697, acc.: 3.22%] [G loss: 0.034824]\n",
      "957 [D loss: 3.034453, acc.: 3.22%] [G loss: 0.034790]\n",
      "958 [D loss: 3.035217, acc.: 3.22%] [G loss: 0.034755]\n",
      "959 [D loss: 3.035961, acc.: 3.22%] [G loss: 0.034721]\n",
      "960 [D loss: 3.036719, acc.: 3.22%] [G loss: 0.034687]\n",
      "961 [D loss: 3.037479, acc.: 3.22%] [G loss: 0.034653]\n",
      "962 [D loss: 3.038219, acc.: 3.22%] [G loss: 0.034618]\n",
      "963 [D loss: 3.038973, acc.: 3.22%] [G loss: 0.034585]\n",
      "964 [D loss: 3.039672, acc.: 3.22%] [G loss: 0.034551]\n",
      "965 [D loss: 3.040395, acc.: 3.22%] [G loss: 0.034517]\n",
      "966 [D loss: 3.041150, acc.: 3.22%] [G loss: 0.034483]\n",
      "967 [D loss: 3.041895, acc.: 3.22%] [G loss: 0.034449]\n",
      "968 [D loss: 3.042631, acc.: 3.22%] [G loss: 0.034416]\n",
      "969 [D loss: 3.043351, acc.: 3.22%] [G loss: 0.034382]\n",
      "970 [D loss: 3.044075, acc.: 3.22%] [G loss: 0.034348]\n",
      "971 [D loss: 3.044805, acc.: 3.22%] [G loss: 0.034315]\n",
      "972 [D loss: 3.045541, acc.: 3.22%] [G loss: 0.034281]\n",
      "973 [D loss: 3.046285, acc.: 3.22%] [G loss: 0.034248]\n",
      "974 [D loss: 3.047047, acc.: 3.22%] [G loss: 0.034215]\n",
      "975 [D loss: 3.047778, acc.: 3.22%] [G loss: 0.034182]\n",
      "976 [D loss: 3.048502, acc.: 3.22%] [G loss: 0.034148]\n",
      "977 [D loss: 3.049250, acc.: 3.22%] [G loss: 0.034115]\n",
      "978 [D loss: 3.050008, acc.: 3.21%] [G loss: 0.034082]\n",
      "979 [D loss: 3.050750, acc.: 3.21%] [G loss: 0.034049]\n",
      "980 [D loss: 3.051497, acc.: 3.21%] [G loss: 0.034016]\n",
      "981 [D loss: 3.052235, acc.: 3.21%] [G loss: 0.033984]\n",
      "982 [D loss: 3.052966, acc.: 3.21%] [G loss: 0.033951]\n",
      "983 [D loss: 3.053718, acc.: 3.21%] [G loss: 0.033918]\n",
      "984 [D loss: 3.054469, acc.: 3.21%] [G loss: 0.033885]\n",
      "985 [D loss: 3.055222, acc.: 3.21%] [G loss: 0.033853]\n",
      "986 [D loss: 3.055961, acc.: 3.21%] [G loss: 0.033820]\n",
      "987 [D loss: 3.056675, acc.: 3.21%] [G loss: 0.033788]\n",
      "988 [D loss: 3.057425, acc.: 3.21%] [G loss: 0.033755]\n",
      "989 [D loss: 3.058173, acc.: 3.21%] [G loss: 0.033723]\n",
      "990 [D loss: 3.058897, acc.: 3.21%] [G loss: 0.033691]\n",
      "991 [D loss: 3.059593, acc.: 3.21%] [G loss: 0.033659]\n",
      "992 [D loss: 3.060309, acc.: 3.21%] [G loss: 0.033626]\n",
      "993 [D loss: 3.061045, acc.: 3.21%] [G loss: 0.033594]\n",
      "994 [D loss: 3.061789, acc.: 3.21%] [G loss: 0.033562]\n",
      "995 [D loss: 3.062515, acc.: 3.22%] [G loss: 0.033530]\n",
      "996 [D loss: 3.063252, acc.: 3.22%] [G loss: 0.033499]\n",
      "997 [D loss: 3.063998, acc.: 3.22%] [G loss: 0.033467]\n",
      "998 [D loss: 3.064722, acc.: 3.22%] [G loss: 0.033435]\n",
      "999 [D loss: 3.065456, acc.: 3.21%] [G loss: 0.033403]\n"
     ]
    }
   ],
   "source": [
    "train(epochs=1000, batch_size=256, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc4b5225-2c65-4e6b-ae6f-1aaff13dafae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/00800.png\n",
      "images/00400.png\n",
      "images/00600.png\n",
      "images/00200.png\n",
      "images/00000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8d/v29lfxt162sblcjlh7x93vkm0000gp/T/ipykernel_73022/1404932973.py:11: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  gif_images.append(imageio.imread(path))\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "\n",
    "def compose_gif():\n",
    "    data_dir = 'images'\n",
    "    data_dir = pathlib.Path(data_dir)\n",
    "    paths = list(data_dir.glob('*'))\n",
    "\n",
    "    gif_images = []\n",
    "    for path in paths:\n",
    "        print(path)\n",
    "        gif_images.append(imageio.imread(path))\n",
    "    imageio.mimsave('test.gif', gif_images, fps=2)\n",
    "\n",
    "compose_gif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bb2ffe-ec09-4cd3-979f-6d5e6003a899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
